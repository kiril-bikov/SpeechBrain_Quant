{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e82f08",
   "metadata": {},
   "source": [
    "# Evaluation of PTQ on CRDNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716ac047",
   "metadata": {},
   "source": [
    "## Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce9a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.pretrained import EncoderDecoderASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef96c922",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model = EncoderDecoderASR.from_hparams(source=\"speechbrain/asr-crdnn-rnnlm-librispeech\", savedir=\"pretrained_models/speechbrain/asr-crdnn-rnnlm-librispeech\", run_opts={\"device\":\"cuda\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d305ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dummy_input = torch.rand(1, 580, 40).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e8e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(asr_model.mods.encoder.model,\n",
    "             args=dummy_input,\n",
    "             f=\"asr-crdnn-encoder_librispeech.onnx\",\n",
    "             export_params=True,\n",
    "             opset_version=11,\n",
    "             do_constant_folding=True,\n",
    "             input_names = ['modelInput'],\n",
    "             output_names = ['modelOutput'],\n",
    "             dynamic_axes={'modelInput' : {0 : 'batch_size', 1: 'batch_size'},\n",
    "             'modelOutput' : {0 : 'batch_size'}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6be3855",
   "metadata": {},
   "source": [
    "## Post training quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb2c5247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import logging\n",
    "import speechbrain as sb\n",
    "from speechbrain.utils.distributed import run_on_main, if_main_process\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from speechbrain.dataio.dataloader import SaveableDataLoader\n",
    "from speechbrain.dataio.dataloader import LoopedLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "from pytorch_quantization import tensor_quant\n",
    "from pytorch_quantization.tensor_quant import QuantDescriptor\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import soundfile as sf\n",
    "import resampy\n",
    "import re\n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"]'\n",
    "\n",
    "def remove_special_characters_dev(batch):\n",
    "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower() + \" \"\n",
    "    trimmed_string = batch[\"text\"].strip()\n",
    "    words = trimmed_string.split()\n",
    "    new_text = ' '.join(words[1:])\n",
    "    batch[\"text\"] = new_text\n",
    "    batch[\"input_values\"] = flac_to_array_with_sampling_rate(dev_clean_dict[words[0]]['file_path'])\n",
    "    return batch\n",
    "\n",
    "def flac_to_array_with_sampling_rate(file_path, target_sr=16000):\n",
    "    data, sr = sf.read(file_path, dtype='float32')\n",
    "\n",
    "    if sr != target_sr:\n",
    "        data = resampy.resample(data, sr, target_sr)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Define training procedure\n",
    "class ASR(sb.Brain):\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"\"\"Forward computations from the waveform batches to the output probabilities.\"\"\"\n",
    "        batch = batch.to(self.device)\n",
    "        wavs, wav_lens = batch.sig\n",
    "        tokens_bos, _ = batch.tokens_bos\n",
    "        wavs, wav_lens = wavs.to(self.device), wav_lens.to(self.device)\n",
    "\n",
    "        # Add augmentation if specified\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            if hasattr(self.modules, \"env_corrupt\"):\n",
    "                wavs_noise = self.modules.env_corrupt(wavs, wav_lens)\n",
    "                wavs = torch.cat([wavs, wavs_noise], dim=0)\n",
    "                wav_lens = torch.cat([wav_lens, wav_lens])\n",
    "                tokens_bos = torch.cat([tokens_bos, tokens_bos], dim=0)\n",
    "\n",
    "            if hasattr(self.hparams, \"augmentation\"):\n",
    "                wavs = self.hparams.augmentation(wavs, wav_lens)\n",
    "\n",
    "        # Forward pass\n",
    "        feats = self.hparams.compute_features(wavs)\n",
    "        feats = self.modules.normalize(feats, wav_lens)\n",
    "        x = self.modules.enc(feats.detach())\n",
    "        e_in = self.modules.emb(tokens_bos)  # y_in bos + tokens\n",
    "        h, _ = self.modules.dec(e_in, x, wav_lens)\n",
    "\n",
    "        # Output layer for seq2seq log-probabilities\n",
    "        logits = self.modules.seq_lin(h)\n",
    "        p_seq = self.hparams.log_softmax(logits)\n",
    "\n",
    "        # Compute outputs\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            current_epoch = self.hparams.epoch_counter.current\n",
    "            if current_epoch <= self.hparams.number_of_ctc_epochs:\n",
    "                # Output layer for ctc log-probabilities\n",
    "                logits = self.modules.ctc_lin(x)\n",
    "                p_ctc = self.hparams.log_softmax(logits)\n",
    "                return p_ctc, p_seq, wav_lens\n",
    "            else:\n",
    "                return p_seq, wav_lens\n",
    "        else:\n",
    "            if stage == sb.Stage.VALID:\n",
    "                p_tokens, scores = self.hparams.valid_search(x, wav_lens)\n",
    "            else:\n",
    "                p_tokens, scores = self.hparams.test_search(x, wav_lens)\n",
    "            return p_seq, wav_lens, p_tokens\n",
    "\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "        \"\"\"Computes the loss (CTC+NLL) given predictions and targets.\"\"\"\n",
    "\n",
    "        current_epoch = self.hparams.epoch_counter.current\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            if current_epoch <= self.hparams.number_of_ctc_epochs:\n",
    "                p_ctc, p_seq, wav_lens = predictions\n",
    "            else:\n",
    "                p_seq, wav_lens = predictions\n",
    "        else:\n",
    "            p_seq, wav_lens, predicted_tokens = predictions\n",
    "\n",
    "        ids = batch.id\n",
    "        tokens_eos, tokens_eos_lens = batch.tokens_eos\n",
    "        tokens, tokens_lens = batch.tokens\n",
    "\n",
    "        if hasattr(self.modules, \"env_corrupt\") and stage == sb.Stage.TRAIN:\n",
    "            tokens_eos = torch.cat([tokens_eos, tokens_eos], dim=0)\n",
    "            tokens_eos_lens = torch.cat(\n",
    "                [tokens_eos_lens, tokens_eos_lens], dim=0\n",
    "            )\n",
    "            tokens = torch.cat([tokens, tokens], dim=0)\n",
    "            tokens_lens = torch.cat([tokens_lens, tokens_lens], dim=0)\n",
    "\n",
    "        loss_seq = self.hparams.seq_cost(\n",
    "            p_seq, tokens_eos, length=tokens_eos_lens\n",
    "        )\n",
    "\n",
    "        # Add ctc loss if necessary\n",
    "        if (\n",
    "            stage == sb.Stage.TRAIN\n",
    "            and current_epoch <= self.hparams.number_of_ctc_epochs\n",
    "        ):\n",
    "            loss_ctc = self.hparams.ctc_cost(\n",
    "                p_ctc, tokens, wav_lens, tokens_lens\n",
    "            )\n",
    "            loss = self.hparams.ctc_weight * loss_ctc\n",
    "            loss += (1 - self.hparams.ctc_weight) * loss_seq\n",
    "        else:\n",
    "            loss = loss_seq\n",
    "\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            # Decode token terms to words\n",
    "            predicted_words = [\n",
    "                self.tokenizer.decode_ids(utt_seq).split(\" \")\n",
    "                for utt_seq in predicted_tokens\n",
    "            ]\n",
    "            target_words = [wrd.split(\" \") for wrd in batch.wrd]\n",
    "            self.wer_metric.append(ids, predicted_words, target_words)\n",
    "            self.cer_metric.append(ids, predicted_words, target_words)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def fit_batch(self, batch):\n",
    "        \"\"\"Train the parameters given a single batch in input\"\"\"\n",
    "        predictions = self.compute_forward(batch, sb.Stage.TRAIN)\n",
    "        loss = self.compute_objectives(predictions, batch, sb.Stage.TRAIN)\n",
    "        loss.backward()\n",
    "        if self.check_gradients(loss):\n",
    "            self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        return loss.detach()\n",
    "\n",
    "    def evaluate_batch(self, batch, stage):\n",
    "        \"\"\"Computations needed for validation/test batches\"\"\"\n",
    "        predictions = self.compute_forward(batch, stage=stage)\n",
    "        with torch.no_grad():\n",
    "            loss = self.compute_objectives(predictions, batch, stage=stage)\n",
    "        return loss.detach()\n",
    "\n",
    "    def on_stage_start(self, stage, epoch):\n",
    "        \"\"\"Gets called at the beginning of each epoch\"\"\"\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.cer_metric = self.hparams.cer_computer()\n",
    "            self.wer_metric = self.hparams.error_rate_computer()\n",
    "\n",
    "    def on_stage_end(self, stage, stage_loss, epoch):\n",
    "        \"\"\"Gets called at the end of a epoch.\"\"\"\n",
    "        # Compute/store important stats\n",
    "        stage_stats = {\"loss\": stage_loss}\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            self.train_stats = stage_stats\n",
    "        else:\n",
    "            stage_stats[\"CER\"] = self.cer_metric.summarize(\"error_rate\")\n",
    "            stage_stats[\"WER\"] = self.wer_metric.summarize(\"error_rate\")\n",
    "\n",
    "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
    "        if stage == sb.Stage.VALID:\n",
    "            old_lr, new_lr = self.hparams.lr_annealing(stage_stats[\"WER\"])\n",
    "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                stats_meta={\"epoch\": epoch, \"lr\": old_lr},\n",
    "                train_stats=self.train_stats,\n",
    "                valid_stats=stage_stats,\n",
    "            )\n",
    "            self.checkpointer.save_and_keep_only(\n",
    "                meta={\"WER\": stage_stats[\"WER\"]}, min_keys=[\"WER\"],\n",
    "            )\n",
    "        elif stage == sb.Stage.TEST:\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
    "                test_stats=stage_stats,\n",
    "            )\n",
    "            if if_main_process():\n",
    "                with open(self.hparams.test_wer_file, \"w\") as w:\n",
    "                    self.wer_metric.write_stats(w)\n",
    "\n",
    "    def quant_evaluate(\n",
    "            self,\n",
    "            test_set,\n",
    "            train_set,\n",
    "            max_key=None,\n",
    "            min_key=None,\n",
    "            progressbar=None,\n",
    "            test_loader_kwargs={},\n",
    "            train_loader_kwargs={},\n",
    "    ):\n",
    "\n",
    "        if progressbar is None:\n",
    "            progressbar = not self.noprogressbar\n",
    "\n",
    "        if not (\n",
    "                isinstance(test_set, torch.utils.data.DataLoader)\n",
    "                or isinstance(test_set, LoopedLoader)\n",
    "        ):\n",
    "            test_loader_kwargs[\"ckpt_prefix\"] = None\n",
    "            test_set = self.make_dataloader(\n",
    "                test_set, sb.Stage.TEST, **test_loader_kwargs\n",
    "            )\n",
    "        self.on_evaluate_start(max_key=max_key, min_key=min_key)\n",
    "\n",
    "        quant_dataloader = self.make_dataloader(\n",
    "            train_set, stage=sb.Stage.TRAIN, **train_loader_kwargs\n",
    "        )\n",
    "        quantize(self, quant_dataloader)\n",
    "        self.on_stage_start(sb.Stage.TEST, epoch=None)\n",
    "\n",
    "\n",
    "def dataio_prepare(hparams):\n",
    "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
    "    It also defines the data processing pipeline through user-defined functions.\"\"\"\n",
    "    data_folder = hparams[\"data_folder\"]\n",
    "\n",
    "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
    "        csv_path=hparams[\"train_csv\"], replacements={\"data_root\": data_folder},\n",
    "    )\n",
    "\n",
    "    if hparams[\"sorting\"] == \"ascending\":\n",
    "        # we sort training data to speed up training and get better results.\n",
    "        train_data = train_data.filtered_sorted(sort_key=\"duration\")\n",
    "        # when sorting do not shuffle in dataloader ! otherwise is pointless\n",
    "        hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "    elif hparams[\"sorting\"] == \"descending\":\n",
    "        train_data = train_data.filtered_sorted(\n",
    "            sort_key=\"duration\", reverse=True\n",
    "        )\n",
    "        # when sorting do not shuffle in dataloader ! otherwise is pointless\n",
    "        hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "    elif hparams[\"sorting\"] == \"random\":\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            \"sorting must be random, ascending or descending\"\n",
    "        )\n",
    "\n",
    "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
    "        csv_path=hparams[\"valid_csv\"], replacements={\"data_root\": data_folder},\n",
    "    )\n",
    "    valid_data = valid_data.filtered_sorted(sort_key=\"duration\")\n",
    "\n",
    "    # test is separate\n",
    "    test_datasets = {}\n",
    "    for csv_file in hparams[\"test_csv\"]:\n",
    "        name = Path(csv_file).stem\n",
    "        test_datasets[name] = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
    "            csv_path=csv_file, replacements={\"data_root\": data_folder}\n",
    "        )\n",
    "        test_datasets[name] = test_datasets[name].filtered_sorted(\n",
    "            sort_key=\"duration\"\n",
    "        )\n",
    "\n",
    "    datasets = [train_data, valid_data] + [i for k, i in test_datasets.items()]\n",
    "\n",
    "    # We get the tokenizer as we need it to encode the labels when creating\n",
    "    # mini-batches.\n",
    "    tokenizer = hparams[\"tokenizer\"]\n",
    "\n",
    "    # 2. Define audio pipeline:\n",
    "    @sb.utils.data_pipeline.takes(\"wav\")\n",
    "    @sb.utils.data_pipeline.provides(\"sig\")\n",
    "    def audio_pipeline(wav):\n",
    "        sig = sb.dataio.dataio.read_audio(wav)\n",
    "        return sig\n",
    "\n",
    "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
    "\n",
    "    # 3. Define text pipeline:\n",
    "    @sb.utils.data_pipeline.takes(\"wrd\")\n",
    "    @sb.utils.data_pipeline.provides(\n",
    "        \"wrd\", \"tokens_list\", \"tokens_bos\", \"tokens_eos\", \"tokens\"\n",
    "    )\n",
    "    def text_pipeline(wrd):\n",
    "        yield wrd\n",
    "        tokens_list = tokenizer.encode_as_ids(wrd)\n",
    "        yield tokens_list\n",
    "        tokens_bos = torch.LongTensor([hparams[\"bos_index\"]] + (tokens_list))\n",
    "        yield tokens_bos\n",
    "        tokens_eos = torch.LongTensor(tokens_list + [hparams[\"eos_index\"]])\n",
    "        yield tokens_eos\n",
    "        tokens = torch.LongTensor(tokens_list)\n",
    "        yield tokens\n",
    "\n",
    "    sb.dataio.dataset.add_dynamic_item(datasets, text_pipeline)\n",
    "\n",
    "    # 4. Set output:\n",
    "    sb.dataio.dataset.set_output_keys(\n",
    "        datasets, [\"id\", \"sig\", \"wrd\", \"tokens_bos\", \"tokens_eos\", \"tokens\"],\n",
    "    )\n",
    "    train_batch_sampler = None\n",
    "    valid_batch_sampler = None\n",
    "    if hparams[\"dynamic_batching\"]:\n",
    "        from speechbrain.dataio.sampler import DynamicBatchSampler  # noqa\n",
    "        from speechbrain.dataio.dataloader import SaveableDataLoader  # noqa\n",
    "        from speechbrain.dataio.batch import PaddedBatch  # noqa\n",
    "\n",
    "        dynamic_hparams = hparams[\"dynamic_batch_sampler\"]\n",
    "        hop_size = dynamic_hparams[\"feats_hop_size\"]\n",
    "\n",
    "        num_buckets = dynamic_hparams[\"num_buckets\"]\n",
    "\n",
    "        train_batch_sampler = DynamicBatchSampler(\n",
    "            train_data,\n",
    "            dynamic_hparams[\"max_batch_len\"],\n",
    "            num_buckets=num_buckets,\n",
    "            length_func=lambda x: x[\"duration\"] * (1 / hop_size),\n",
    "            shuffle=dynamic_hparams[\"shuffle_ex\"],\n",
    "            batch_ordering=dynamic_hparams[\"batch_ordering\"],\n",
    "        )\n",
    "\n",
    "        valid_batch_sampler = DynamicBatchSampler(\n",
    "            valid_data,\n",
    "            dynamic_hparams[\"max_batch_len\"],\n",
    "            num_buckets=num_buckets,\n",
    "            length_func=lambda x: x[\"duration\"] * (1 / hop_size),\n",
    "            shuffle=dynamic_hparams[\"shuffle_ex\"],\n",
    "            batch_ordering=dynamic_hparams[\"batch_ordering\"],\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        train_data,\n",
    "        valid_data,\n",
    "        test_datasets,\n",
    "        train_batch_sampler,\n",
    "        valid_batch_sampler,\n",
    "    )\n",
    "\n",
    "\n",
    "class QuantEmb(nn.Embedding):\n",
    "    def __init__(self, num_embeddings, embedding_dim, padding_idx=None, quant_desc_weight=None):\n",
    "        super(QuantEmb, self).__init__(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
    "        self.init_quantizer(quant_desc_weight)\n",
    "\n",
    "    def init_quantizer(self, quant_desc_weight):\n",
    "        if quant_desc_weight is not None:\n",
    "            self._weight_quantizer = quant_nn.modules.tensor_quantizer.TensorQuantizer(quant_desc_weight)\n",
    "        else:\n",
    "            self._weight_quantizer = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self._weight_quantizer is not None:\n",
    "            quant_weight = self._weight_quantizer(self.weight)\n",
    "        else:\n",
    "            quant_weight = self.weight\n",
    "\n",
    "        return F.embedding(\n",
    "            input, quant_weight, padding_idx=self.padding_idx, max_norm=self.max_norm,\n",
    "            norm_type=self.norm_type, scale_grad_by_freq=self.scale_grad_by_freq, sparse=self.sparse\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def weight_quantizer(self):\n",
    "        return self._weight_quantizer\n",
    "\n",
    "\n",
    "def QuantConv(conv_layer, quant_desc_input, quant_desc_weight):\n",
    "    quantized_conv_layer = quant_nn.QuantConv2d(\n",
    "        in_channels=conv_layer.in_channels,\n",
    "        out_channels=conv_layer.out_channels,\n",
    "        kernel_size=conv_layer.kernel_size,\n",
    "        stride=conv_layer.stride,\n",
    "        padding=conv_layer.padding,\n",
    "        dilation=conv_layer.dilation,\n",
    "        groups=conv_layer.groups,\n",
    "        bias=conv_layer.bias is not None,\n",
    "        padding_mode=conv_layer.padding_mode,\n",
    "        quant_desc_input=quant_desc_input,\n",
    "        quant_desc_weight=quant_desc_weight\n",
    "    )\n",
    "\n",
    "    quantized_conv_layer.weight = conv_layer.weight\n",
    "\n",
    "    if conv_layer.bias is not None:\n",
    "        quantized_conv_layer.bias = conv_layer.bias\n",
    "\n",
    "    return quantized_conv_layer\n",
    "\n",
    "\n",
    "def QuantLinear(linear_layer, quant_desc_input, quant_desc_weight):\n",
    "    quantized_linear_layer = quant_nn.QuantLinear(\n",
    "        in_features=linear_layer.in_features,\n",
    "        out_features=linear_layer.out_features,\n",
    "        bias=linear_layer.bias is not None,\n",
    "        quant_desc_input=quant_desc_input,\n",
    "        quant_desc_weight=quant_desc_weight\n",
    "    )\n",
    "\n",
    "    quantized_linear_layer.weight = linear_layer.weight\n",
    "\n",
    "    if linear_layer.bias is not None:\n",
    "        quantized_linear_layer.bias = linear_layer.bias\n",
    "\n",
    "    return quantized_linear_layer\n",
    "\n",
    "def QuantMaxPool2d(maxpool_layer, quant_desc_input):\n",
    "    quantized_maxpool_layer = quant_nn.QuantMaxPool2d(\n",
    "        kernel_size=maxpool_layer.kernel_size,\n",
    "        stride=maxpool_layer.stride,\n",
    "        padding=maxpool_layer.padding,\n",
    "        dilation=maxpool_layer.dilation,\n",
    "        return_indices=maxpool_layer.return_indices,\n",
    "        ceil_mode=maxpool_layer.ceil_mode,\n",
    "        quant_desc_input=quant_desc_input\n",
    "    )\n",
    "\n",
    "    return quantized_maxpool_layer\n",
    "\n",
    "def QuantLSTM(lstm_layer, quant_desc_input, quant_desc_weight):\n",
    "    quantized_lstm_layer = quant_nn.QuantLSTM(\n",
    "        input_size=lstm_layer.input_size,\n",
    "        hidden_size=lstm_layer.hidden_size,\n",
    "        num_layers=lstm_layer.num_layers,\n",
    "        bias=lstm_layer.bias,\n",
    "        batch_first=lstm_layer.batch_first,\n",
    "        dropout=lstm_layer.dropout,\n",
    "        bidirectional=lstm_layer.bidirectional,\n",
    "        quant_desc_input=quant_desc_input,\n",
    "        quant_desc_weight=quant_desc_weight\n",
    "    )\n",
    "\n",
    "    # Copy weights for each layer\n",
    "    for layer in range(lstm_layer.num_layers):\n",
    "        # Forward direction\n",
    "        setattr(quantized_lstm_layer, f'weight_ih_l{layer}', getattr(lstm_layer, f'weight_ih_l{layer}'))\n",
    "        setattr(quantized_lstm_layer, f'weight_hh_l{layer}', getattr(lstm_layer, f'weight_hh_l{layer}'))\n",
    "        if getattr(lstm_layer, f'bias_ih_l{layer}', None) is not None:\n",
    "            setattr(quantized_lstm_layer, f'bias_ih_l{layer}', getattr(lstm_layer, f'bias_ih_l{layer}'))\n",
    "        if getattr(lstm_layer, f'bias_hh_l{layer}', None) is not None:\n",
    "            setattr(quantized_lstm_layer, f'bias_hh_l{layer}', getattr(lstm_layer, f'bias_hh_l{layer}'))\n",
    "\n",
    "        # Backward direction (reverse)\n",
    "        setattr(quantized_lstm_layer, f'weight_ih_l{layer}_reverse', getattr(lstm_layer, f'weight_ih_l{layer}_reverse'))\n",
    "        setattr(quantized_lstm_layer, f'weight_hh_l{layer}_reverse', getattr(lstm_layer, f'weight_hh_l{layer}_reverse'))\n",
    "        if getattr(lstm_layer, f'bias_ih_l{layer}_reverse', None) is not None:\n",
    "            setattr(quantized_lstm_layer, f'bias_ih_l{layer}_reverse', getattr(lstm_layer, f'bias_ih_l{layer}_reverse'))\n",
    "        if getattr(lstm_layer, f'bias_hh_l{layer}_reverse', None) is not None:\n",
    "            setattr(quantized_lstm_layer, f'bias_hh_l{layer}_reverse', getattr(lstm_layer, f'bias_hh_l{layer}_reverse'))\n",
    "\n",
    "    return quantized_lstm_layer\n",
    "\n",
    "def QuantEmbedding(embedding_layer, quant_desc_weight):\n",
    "\n",
    "    quantized_embedding_layer = QuantEmb(\n",
    "        num_embeddings=embedding_layer.num_embeddings,\n",
    "        embedding_dim=embedding_layer.embedding_dim,\n",
    "        padding_idx=embedding_layer.padding_idx,\n",
    "        quant_desc_weight=quant_desc_weight\n",
    "    )\n",
    "\n",
    "    quantized_embedding_layer.weight = embedding_layer.weight\n",
    "\n",
    "    return quantized_embedding_layer\n",
    "\n",
    "\n",
    "\n",
    "def QuantGRUCell(gru_cell, quant_desc_input, quant_desc_weight):\n",
    "    quantized_gru_cell = quant_nn.QuantGRUCell(\n",
    "        input_size=gru_cell.input_size,\n",
    "        hidden_size=gru_cell.hidden_size,\n",
    "        bias=gru_cell.bias,\n",
    "        quant_desc_input=quant_desc_input,\n",
    "        quant_desc_weight=quant_desc_weight\n",
    "    )\n",
    "\n",
    "    # Copy weights and biases\n",
    "    quantized_gru_cell.weight_ih = gru_cell.weight_ih\n",
    "    quantized_gru_cell.weight_hh = gru_cell.weight_hh\n",
    "    if gru_cell.bias:\n",
    "        quantized_gru_cell.bias_ih = gru_cell.bias_ih\n",
    "        quantized_gru_cell.bias_hh = gru_cell.bias_hh\n",
    "\n",
    "    return quantized_gru_cell\n",
    "\n",
    "def QuantConv1d(conv1d_layer, quant_desc_input, quant_desc_weight):\n",
    "    quantized_conv1d_layer = quant_nn.QuantConv1d(\n",
    "        in_channels=conv1d_layer.in_channels,\n",
    "        out_channels=conv1d_layer.out_channels,\n",
    "        kernel_size=conv1d_layer.kernel_size,\n",
    "        stride=conv1d_layer.stride,\n",
    "        padding=conv1d_layer.padding,\n",
    "        dilation=conv1d_layer.dilation,\n",
    "        groups=conv1d_layer.groups,\n",
    "        bias=conv1d_layer.bias is not None,\n",
    "        padding_mode=conv1d_layer.padding_mode,\n",
    "        quant_desc_input=quant_desc_input,\n",
    "        quant_desc_weight=quant_desc_weight\n",
    "    )\n",
    "\n",
    "    # Copy weights and biases from the original conv1d_layer\n",
    "    quantized_conv1d_layer.weight = conv1d_layer.weight\n",
    "    if conv1d_layer.bias is not None:\n",
    "        quantized_conv1d_layer.bias = conv1d_layer.bias\n",
    "\n",
    "    return quantized_conv1d_layer\n",
    "\n",
    "def QuantLayerNorm(layer_norm_layer, quant_desc_input):\n",
    "    quantized_layer_norm = quant_nn.QuantLayerNorm(\n",
    "        normalized_shape=layer_norm_layer.normalized_shape,\n",
    "        eps=layer_norm_layer.eps,\n",
    "        elementwise_affine=layer_norm_layer.elementwise_affine,\n",
    "        quant_desc_input=quant_desc_input\n",
    "    )\n",
    "\n",
    "    if layer_norm_layer.elementwise_affine:\n",
    "        quantized_layer_norm.weight = layer_norm_layer.weight\n",
    "        quantized_layer_norm.bias = layer_norm_layer.bias\n",
    "\n",
    "    return quantized_layer_norm\n",
    "\n",
    "def QuantBatchNorm1d(batchnorm1d_layer, quant_desc_input):\n",
    "    quantized_batchnorm1d_layer = quant_nn.QuantBatchNorm1d(\n",
    "        num_features=batchnorm1d_layer.num_features,\n",
    "        eps=batchnorm1d_layer.eps,\n",
    "        momentum=batchnorm1d_layer.momentum,\n",
    "        affine=batchnorm1d_layer.affine,\n",
    "        track_running_stats=batchnorm1d_layer.track_running_stats,\n",
    "        quant_desc_input=quant_desc_input\n",
    "    )\n",
    "\n",
    "    # Copy weights and biases from the original batchnorm1d_layer\n",
    "    if batchnorm1d_layer.affine:\n",
    "        quantized_batchnorm1d_layer.weight = batchnorm1d_layer.weight\n",
    "        quantized_batchnorm1d_layer.bias = batchnorm1d_layer.bias\n",
    "\n",
    "    if batchnorm1d_layer.track_running_stats:\n",
    "        quantized_batchnorm1d_layer.running_mean = batchnorm1d_layer.running_mean\n",
    "        quantized_batchnorm1d_layer.running_var = batchnorm1d_layer.running_var\n",
    "\n",
    "    return quantized_batchnorm1d_layer\n",
    "\n",
    "def quantize_layer(layer, flag = False):\n",
    "    if flag:\n",
    "        input_quant_descriptor = QuantDescriptor(num_bits=16)\n",
    "    else:\n",
    "        input_quant_descriptor = QuantDescriptor(num_bits=8)\n",
    "    conv_weight_quant_descriptor = QuantDescriptor(num_bits=4, axis =(0))\n",
    "    weight_quant_descriptor = QuantDescriptor(num_bits=4)\n",
    "    input_quant_descriptor = QuantDescriptor(num_bits=32)\n",
    "\n",
    "\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        return QuantConv(layer, quant_desc_input=input_quant_descriptor,\n",
    "                         quant_desc_weight=conv_weight_quant_descriptor)\n",
    "    elif isinstance(layer, torch.nn.Linear):\n",
    "        return QuantLinear(layer, quant_desc_input=input_quant_descriptor,\n",
    "                           quant_desc_weight=weight_quant_descriptor)\n",
    "    elif isinstance(layer, torch.nn.MaxPool2d):\n",
    "        return QuantMaxPool2d(layer, quant_desc_input=input_quant_descriptor)\n",
    "    elif isinstance(layer, torch.nn.LSTM):\n",
    "        return layer\n",
    "    elif isinstance(layer, torch.nn.Embedding):\n",
    "        return QuantEmbedding(layer, quant_desc_weight=weight_quant_descriptor)\n",
    "    elif isinstance(layer, torch.nn.GRUCell):\n",
    "        return layer\n",
    "    elif isinstance(layer, torch.nn.Conv1d):\n",
    "        return QuantConv1d(layer, quant_desc_input=input_quant_descriptor,\n",
    "                           quant_desc_weight=conv_weight_quant_descriptor)\n",
    "    elif isinstance(layer, torch.nn.LayerNorm):\n",
    "        return layer\n",
    "    elif isinstance(layer, torch.nn.BatchNorm1d):\n",
    "        return layer\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def collect_stats(quantized_layers, data_loader, num_batches, model):\n",
    "    # Enable calibrators\n",
    "    for layer in quantized_layers:\n",
    "        if hasattr(layer, 'input_quantizer') and layer.input_quantizer._calibrator is not None:\n",
    "            layer.input_quantizer.disable_quant()\n",
    "            layer.input_quantizer.enable_calib()\n",
    "        if hasattr(layer, 'weight_quantizer') and layer.weight_quantizer._calibrator is not None:\n",
    "            layer.weight_quantizer.disable_quant()\n",
    "            layer.weight_quantizer.enable_calib()\n",
    "\n",
    "    # Feed data through the network\n",
    "    for i, batch in tqdm(enumerate(data_loader), total=num_batches):\n",
    "        print(batch)\n",
    "        loss = model.compute_forward(batch, sb.Stage.TEST)\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        continue\n",
    "\n",
    "    # Disable calibrators\n",
    "    for layer in quantized_layers:\n",
    "        if hasattr(layer, 'input_quantizer') and layer.input_quantizer._calibrator is not None:\n",
    "            layer.input_quantizer.enable_quant()\n",
    "            layer.input_quantizer.disable_calib()\n",
    "        if hasattr(layer, 'weight_quantizer') and layer.weight_quantizer._calibrator is not None:\n",
    "            layer.weight_quantizer.enable_quant()\n",
    "            layer.weight_quantizer.disable_calib()\n",
    "\n",
    "\n",
    "def compute_amax(quantized_layers, **kwargs):\n",
    "    # Load calibration results for each quantized layer\n",
    "    for layer in quantized_layers:\n",
    "        if hasattr(layer, 'input_quantizer') and layer.input_quantizer._calibrator is not None:\n",
    "            if isinstance(layer.input_quantizer._calibrator, calib.MaxCalibrator):\n",
    "                layer.input_quantizer.load_calib_amax()\n",
    "            else:\n",
    "                layer.input_quantizer.load_calib_amax(**kwargs)\n",
    "\n",
    "        if hasattr(layer, 'weight_quantizer') and layer.weight_quantizer._calibrator is not None:\n",
    "            if isinstance(layer.weight_quantizer._calibrator, calib.MaxCalibrator):\n",
    "                layer.weight_quantizer.load_calib_amax()\n",
    "            else:\n",
    "                layer.weight_quantizer.load_calib_amax(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def quantize(asr, train_data):\n",
    "\n",
    "    #encoder\n",
    "\n",
    "    #CNN\n",
    "\n",
    "    b0_conv1 = asr.modules['enc']['CNN']['block_0']['conv_1'].conv\n",
    "    b0_conv2 = asr.modules['enc']['CNN']['block_0']['conv_2'].conv\n",
    "\n",
    "    b1_conv1 = asr.modules['enc']['CNN']['block_1']['conv_1'].conv\n",
    "    b1_conv2 = asr.modules['enc']['CNN']['block_1']['conv_1'].conv\n",
    "\n",
    "    b0_pool = asr.modules['enc']['CNN']['block_0']['pooling'].pool_layer\n",
    "    b1_pool = asr.modules['enc']['CNN']['block_1']['pooling'].pool_layer\n",
    "\n",
    "    b1_norm1 = asr.modules['enc']['CNN']['block_1']['norm_1'].norm\n",
    "    b1_norm2 = asr.modules['enc']['CNN']['block_1']['norm_2'].norm\n",
    "\n",
    "    #DNN\n",
    "    b0_lin1 = asr.modules['enc']['DNN']['block_0']['linear'].w\n",
    "    b1_lin1 = asr.modules['enc']['DNN']['block_1']['linear'].w\n",
    "\n",
    "    #RNN\n",
    "    lstm = asr.modules['enc']['RNN'].rnn\n",
    "    #Embedding\n",
    "    emb = asr.modules['emb'].Embedding.weight\n",
    "\n",
    "    #decoder\n",
    "    proj = asr.modules['dec'].proj\n",
    "    gru_cell = asr.modules['dec'].rnn.rnn_cells[0]\n",
    "\n",
    "\n",
    "    ctc_lin = asr.modules['ctc_lin'].w\n",
    "    seq_lin = asr.modules['seq_lin'].w\n",
    "\n",
    "\n",
    "    quantized_layers = []\n",
    "\n",
    "    asr.modules['enc']['CNN']['block_0']['conv_1'].conv = quantize_layer(\n",
    "        asr.modules['enc']['CNN']['block_0']['conv_1'].conv)\n",
    "    quantized_layers.append(asr.modules['enc']['CNN']['block_0']['conv_1'].conv)\n",
    "    asr.modules['enc']['CNN']['block_0']['conv_2'].conv = quantize_layer(\n",
    "        asr.modules['enc']['CNN']['block_0']['conv_2'].conv)\n",
    "    quantized_layers.append(asr.modules['enc']['CNN']['block_0']['conv_2'].conv)\n",
    "\n",
    "    asr.modules['enc']['CNN']['block_1']['conv_1'].conv = quantize_layer(\n",
    "        asr.modules['enc']['CNN']['block_1']['conv_1'].conv)\n",
    "    quantized_layers.append(asr.modules['enc']['CNN']['block_1']['conv_1'].conv)\n",
    "    asr.modules['enc']['CNN']['block_1']['conv_2'].conv = quantize_layer(\n",
    "        asr.modules['enc']['CNN']['block_1']['conv_2'].conv, flag = True)\n",
    "    quantized_layers.append(asr.modules['enc']['CNN']['block_1']['conv_2'].conv)\n",
    "\n",
    "    asr.modules['enc']['CNN']['block_0']['pooling'].pool_layer = quantize_layer(\n",
    "        asr.modules['enc']['CNN']['block_0']['pooling'].pool_layer)\n",
    "    quantized_layers.append(asr.modules['enc']['CNN']['block_0']['pooling'].pool_layer)\n",
    "    asr.modules['enc']['CNN']['block_1']['pooling'].pool_layer = quantize_layer(\n",
    "        asr.modules['enc']['CNN']['block_1']['pooling'].pool_layer)\n",
    "    quantized_layers.append(asr.modules['enc']['CNN']['block_1']['pooling'].pool_layer)\n",
    "\n",
    "\n",
    "    asr.modules['enc']['CNN']['block_0']['norm_1'].norm = quantize_layer(asr.modules['enc']['CNN']['block_0']['norm_1'].norm)\n",
    "    quantized_layers.append(asr.modules['enc']['CNN']['block_0']['norm_1'].norm)\n",
    "    asr.modules['enc']['CNN']['block_0']['norm_2'].norm = quantize_layer(asr.modules['enc']['CNN']['block_0']['norm_2'].norm)\n",
    "    quantized_layers.append(asr.modules['enc']['CNN']['block_0']['norm_2'].norm)\n",
    "\n",
    "    asr.modules['enc']['CNN']['block_1']['norm_1'].norm = quantize_layer(asr.modules['enc']['CNN']['block_1']['norm_1'].norm)\n",
    "    quantized_layers.append(asr.modules['enc']['CNN']['block_1']['norm_1'].norm)\n",
    "    asr.modules['enc']['CNN']['block_1']['norm_2'].norm = quantize_layer(asr.modules['enc']['CNN']['block_1']['norm_2'].norm)\n",
    "    quantized_layers.append(asr.modules['enc']['CNN']['block_1']['norm_2'].norm)\n",
    "\n",
    "\n",
    "    asr.modules['enc']['DNN']['block_0']['linear'].w = quantize_layer(asr.modules['enc']['DNN']['block_0']['linear'].w)\n",
    "    quantized_layers.append(asr.modules['enc']['DNN']['block_0']['linear'].w)\n",
    "    asr.modules['enc']['DNN']['block_1']['linear'].w = quantize_layer(asr.modules['enc']['DNN']['block_1']['linear'].w)\n",
    "    quantized_layers.append(asr.modules['enc']['DNN']['block_1']['linear'].w)\n",
    "\n",
    "\n",
    "    asr.modules['enc']['DNN']['block_0']['norm'].norm = quantize_layer(asr.modules['enc']['DNN']['block_0']['norm'].norm)\n",
    "    quantized_layers.append(asr.modules['enc']['DNN']['block_0']['norm'].norm)\n",
    "\n",
    "    asr.modules['enc']['DNN']['block_1']['norm'].norm = quantize_layer(asr.modules['enc']['DNN']['block_1']['norm'].norm)\n",
    "    quantized_layers.append(asr.modules['enc']['DNN']['block_1']['norm'].norm)\n",
    "\n",
    "    asr.modules['enc']['RNN'].rnn = quantize_layer(asr.modules['enc']['RNN'].rnn)\n",
    "    quantized_layers.append(asr.modules['enc']['RNN'].rnn)\n",
    "\n",
    "    asr_brain.modules['enc'].time_pooling.pool_layer = quantize_layer(asr_brain.modules['enc'].time_pooling.pool_layer)\n",
    "\n",
    "\n",
    "    asr.modules['dec'].rnn.rnn_cells[0] = quantize_layer(asr.modules['dec'].rnn.rnn_cells[0])\n",
    "    quantized_layers.append(asr.modules['dec'].rnn.rnn_cells[0])\n",
    "    asr.modules['dec'].proj = quantize_layer(asr.modules['dec'].proj)\n",
    "    quantized_layers.append(asr.modules['dec'].proj)\n",
    "\n",
    "    asr.modules['dec'].attn.conv_loc = quantize_layer(asr.modules['dec'].attn.conv_loc)\n",
    "    quantized_layers.append(asr.modules['dec'].attn.conv_loc)\n",
    "    asr.modules['dec'].attn.mlp_attn = quantize_layer( asr.modules['dec'].attn.mlp_attn)\n",
    "    quantized_layers.append(asr.modules['dec'].attn.mlp_attn)\n",
    "    asr.modules['dec'].attn.mlp_dec = quantize_layer(asr.modules['dec'].attn.mlp_dec)\n",
    "    quantized_layers.append(asr.modules['dec'].attn.mlp_dec)\n",
    "    asr.modules['dec'].attn.mlp_enc = quantize_layer(asr.modules['dec'].attn.mlp_enc)\n",
    "    quantized_layers.append(asr.modules['dec'].attn.mlp_enc)\n",
    "    asr.modules['dec'].attn.mlp_loc = quantize_layer(asr.modules['dec'].attn.mlp_loc)\n",
    "    quantized_layers.append(asr.modules['dec'].attn.mlp_loc)\n",
    "    asr.modules['dec'].attn.mlp_out = quantize_layer(asr.modules['dec'].attn.mlp_out)\n",
    "    quantized_layers.append(asr.modules['dec'].attn.mlp_out)\n",
    "\n",
    "    asr.modules['seq_lin'].w = quantize_layer(asr.modules['seq_lin'].w)\n",
    "    quantized_layers.append(asr.modules['seq_lin'].w)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        collect_stats(quantized_layers, train_data, num_batches=8, model=asr)\n",
    "        compute_amax(quantized_layers, method=\"mse\")\n",
    "\n",
    "    asr.modules.to(asr.device)\n",
    "\n",
    "    for module in asr.modules:\n",
    "        if hasattr(asr.modules[module], \"to\"):\n",
    "            asr.modules[module] = asr.modules[module].to(asr.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "523d541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_file = 'hparams/train_BPE_1000.yaml'\n",
    "run_opts = {\"device\":\"cuda\"}\n",
    "overrides = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ea106",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.utils.distributed.ddp_init_group(run_opts)\n",
    "\n",
    "with open(hparams_file) as fin:\n",
    "    hparams = load_hyperpyyaml(fin, overrides)\n",
    "\n",
    "sb.create_experiment_directory(\n",
    "    experiment_directory=hparams[\"output_folder\"],\n",
    "    hyperparams_to_save=hparams_file,\n",
    "    overrides=overrides,\n",
    ")\n",
    "\n",
    "from librispeech_prepare import prepare_librispeech  # noqa\n",
    "\n",
    "run_on_main(\n",
    "    prepare_librispeech,\n",
    "    kwargs={\n",
    "        \"data_folder\": hparams[\"data_folder\"],\n",
    "        \"tr_splits\": hparams[\"train_splits\"],\n",
    "        \"dev_splits\": hparams[\"dev_splits\"],\n",
    "        \"te_splits\": hparams[\"test_splits\"],\n",
    "        \"save_folder\": hparams[\"output_folder\"],\n",
    "        \"merge_lst\": hparams[\"train_splits\"],\n",
    "        \"merge_name\": \"train.csv\",\n",
    "        \"skip_prep\": hparams[\"skip_prep\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "# here we create the datasets objects as well as tokenization and encoding\n",
    "(\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    test_datasets,\n",
    "    train_bsampler,\n",
    "    valid_bsampler,\n",
    ") = dataio_prepare(hparams)\n",
    "\n",
    "run_on_main(hparams[\"pretrainer\"].collect_files)\n",
    "hparams[\"pretrainer\"].load_collected(device=run_opts[\"device\"])\n",
    "\n",
    "asr_brain = ASR(\n",
    "    modules=hparams[\"modules\"],\n",
    "    opt_class=hparams[\"opt_class\"],\n",
    "    hparams=hparams,\n",
    "    run_opts=run_opts,\n",
    "    checkpointer=hparams[\"checkpointer\"],\n",
    ")\n",
    "\n",
    "asr_brain.tokenizer = hparams[\"tokenizer\"]\n",
    "train_dataloader_opts = hparams[\"train_dataloader_opts\"]\n",
    "valid_dataloader_opts = hparams[\"valid_dataloader_opts\"]\n",
    "\n",
    "if train_bsampler is not None:\n",
    "    train_dataloader_opts = {\"batch_sampler\": train_bsampler}\n",
    "if valid_bsampler is not None:\n",
    "    valid_dataloader_opts = {\"batch_sampler\": valid_bsampler}\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# Testing\n",
    "k = 'test-clean'\n",
    "if not os.path.exists(hparams[\"output_wer_folder\"]):\n",
    "    os.makedirs(hparams[\"output_wer_folder\"])\n",
    "asr_brain.hparams.test_wer_file = os.path.join(\n",
    "        hparams[\"output_wer_folder\"], f\"wer_{k}.txt\"\n",
    "    )\n",
    "\n",
    "asr_brain.quant_evaluate(\n",
    "        test_datasets[k], train_data,\n",
    "        test_loader_kwargs=hparams[\"test_dataloader_opts\"],\n",
    "        train_loader_kwargs=hparams[\"train_dataloader_opts\"],\n",
    "        min_key=\"WER\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4827539",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_brain.modules.enc = asr_brain.modules.enc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4682ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(asr_brain.modules.enc,\n",
    "             args=dummy_input,\n",
    "             f=\"asr-crdnn-encoder_librispeech_ptq.onnx\",\n",
    "             export_params=True,\n",
    "             opset_version=11,\n",
    "             do_constant_folding=True,\n",
    "             input_names = ['modelInput'],\n",
    "             output_names = ['modelOutput'],\n",
    "             dynamic_axes={'modelInput' : {0 : 'batch_size', 1: 'batch_size'},\n",
    "             'modelOutput' : {0 : 'batch_size'}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab29266d",
   "metadata": {},
   "source": [
    "## Base vs Quantized Model Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e540cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d4726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import tensorrt\n",
    "ort.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521071ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_sess_base = ort.InferenceSession('asr-crdnn-encoder_librispeech.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f4103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_sess_quantized = ort.InferenceSession('asr-crdnn-encoder_librispeech_ptq.onnx', providers=['TensorrtExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93c64ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import time\n",
    "\n",
    "with torch.no_grad():\n",
    "        input_values = torch.tensor(dummy_input, device=\"cpu\")\n",
    "        input_values = input_values.clone().detach().numpy().astype(numpy.float32)\n",
    "        starttime = time.time()\n",
    "        ort_sess_base.run(['modelOutput'], {'modelInput': input_values})\n",
    "        base_model_time = time.time() - starttime\n",
    "        print(f'Base Model inference time {base_model_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5c69cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "        starttime = time.time()\n",
    "        ort_sess_base.run(['modelOutput'], {'modelInput': input_values})\n",
    "        quantized_model_time = time.time() - starttime\n",
    "        print(f'Quantized Model inference time {quantized_model_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c785728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Quantized Model inference time {quantized_model_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d951a988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = ['Base', 'Quantized']\n",
    "inference_times = [base_model_time, quantized_model_time]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(models, inference_times)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Inference Time (seconds)')\n",
    "plt.title('Inference Time Comparison: Base CRDNN vs Quantized CRDNN')\n",
    "plt.ylim(0, max(inference_times) * 1.1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
