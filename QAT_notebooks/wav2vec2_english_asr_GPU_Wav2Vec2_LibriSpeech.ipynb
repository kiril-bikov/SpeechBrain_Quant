{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "590528f4",
   "metadata": {},
   "source": [
    "# Training-Aware Quantization of SpeechBrain Wav2Vec2 on GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91676ae2",
   "metadata": {},
   "source": [
    "This notebook provides code for training-aware quantization of the speechbrain/asr-wav2vec2-librispeech model. We compare fine-tuned base model and quantization-aware trained model on the librispeech dataset. Training is performed for two epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ba4a75",
   "metadata": {},
   "source": [
    "First, we define the ASR class and write helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59b081d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import logging\n",
    "import speechbrain as sb\n",
    "from speechbrain.utils.distributed import run_on_main, if_main_process\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "from pathlib import Path\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ASR(sb.Brain):\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"\"\"Forward computations from the waveform batches to the output probabilities.\"\"\"\n",
    "        batch = batch.to(self.device)\n",
    "        wavs, wav_lens = batch.sig\n",
    "        wavs, wav_lens = wavs.to(self.device), wav_lens.to(self.device)\n",
    "        if hasattr(self.modules, \"downsampler\"):\n",
    "            wavs = self.modules.downsampler(wavs)\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            if hasattr(self.modules, \"env_corrupt\"):\n",
    "                wavs_noise = self.modules.env_corrupt(wavs, wav_lens)\n",
    "                wavs = torch.cat([wavs, wavs_noise], dim=0)\n",
    "                wav_lens = torch.cat([wav_lens, wav_lens])\n",
    "\n",
    "            if hasattr(self.hparams, \"augmentation\"):\n",
    "                wavs = self.hparams.augmentation(wavs, wav_lens)\n",
    "\n",
    "        if hasattr(self.modules, \"extractor\"): \n",
    "            latents = self.modules.extractor(wavs)\n",
    "            feats = self.modules.encoder_wrapper(latents, wav_lens=wav_lens)[\n",
    "                \"embeddings\"\n",
    "            ]\n",
    "        else:\n",
    "            feats = self.modules.wav2vec2(wavs, wav_lens)\n",
    "\n",
    "        x = self.modules.enc(feats)\n",
    "\n",
    "        p_tokens = None\n",
    "        logits = self.modules.ctc_lin(x)\n",
    "\n",
    "        if hasattr(self.hparams, \"upsampling\") and self.hparams.upsampling:\n",
    "            logits = logits.view(\n",
    "                logits.shape[0], -1, self.hparams.output_neurons\n",
    "            )\n",
    "\n",
    "        p_ctc = self.hparams.log_softmax(logits)\n",
    "        if stage == sb.Stage.VALID or (\n",
    "            stage == sb.Stage.TEST and not self.hparams.use_language_modelling\n",
    "        ):\n",
    "\n",
    "            p_tokens = sb.decoders.ctc_greedy_decode(\n",
    "                p_ctc, wav_lens, blank_id=self.hparams.blank_index\n",
    "            )\n",
    "        return p_ctc, wav_lens, p_tokens\n",
    "\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "        \"\"\"Computes the loss (CTC+NLL) given predictions and targets.\"\"\"\n",
    "\n",
    "        p_ctc, wav_lens, predicted_tokens = predictions\n",
    "\n",
    "        ids = batch.id\n",
    "        tokens, tokens_lens = batch.tokens\n",
    "\n",
    "        if hasattr(self.modules, \"env_corrupt\") and stage == sb.Stage.TRAIN:\n",
    "            tokens = torch.cat([tokens, tokens], dim=0)\n",
    "            tokens_lens = torch.cat([tokens_lens, tokens_lens], dim=0)\n",
    "\n",
    "        loss_ctc = self.hparams.ctc_cost(p_ctc, tokens, wav_lens, tokens_lens)\n",
    "        loss = loss_ctc\n",
    "\n",
    "        if stage == sb.Stage.VALID:\n",
    "            predicted_words = [\n",
    "                \"\".join(self.tokenizer.decode_ndim(utt_seq)).split(\" \")\n",
    "                for utt_seq in predicted_tokens\n",
    "            ]\n",
    "            target_words = [wrd.split(\" \") for wrd in batch.wrd]\n",
    "            self.wer_metric.append(ids, predicted_words, target_words)\n",
    "            self.cer_metric.append(ids, predicted_words, target_words)\n",
    "        if stage == sb.Stage.TEST:\n",
    "            if self.hparams.use_language_modelling:\n",
    "                predicted_words = []\n",
    "                for logs in p_ctc:\n",
    "                    text = decoder.decode(logs.detach().cpu().numpy())\n",
    "                    predicted_words.append(text.split(\" \"))\n",
    "            else:\n",
    "                predicted_words = [\n",
    "                    \"\".join(self.tokenizer.decode_ndim(utt_seq)).split(\" \")\n",
    "                    for utt_seq in predicted_tokens\n",
    "                ]\n",
    "            target_words = [wrd.split(\" \") for wrd in batch.wrd]\n",
    "            self.wer_metric.append(ids, predicted_words, target_words)\n",
    "            self.cer_metric.append(ids, predicted_words, target_words)\n",
    "        return loss\n",
    "\n",
    "    def fit_batch(self, batch):\n",
    "        should_step = self.step % self.grad_accumulation_factor == 0\n",
    "\n",
    "        if self.auto_mix_prec:\n",
    "            self.wav2vec_optimizer.zero_grad()\n",
    "            self.model_optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                with self.no_sync():\n",
    "                    outputs = self.compute_forward(batch, sb.Stage.TRAIN)\n",
    "                loss = self.compute_objectives(outputs, batch, sb.Stage.TRAIN)\n",
    "            with self.no_sync(not should_step):\n",
    "                self.scaler.scale(\n",
    "                    loss / self.grad_accumulation_factor\n",
    "                ).backward()\n",
    "            if should_step:\n",
    "                if not self.hparams.freeze_wav2vec:\n",
    "                    self.scaler.unscale_(self.wav2vec_optimizer)\n",
    "                self.scaler.unscale_(self.model_optimizer)\n",
    "                if self.check_gradients(loss):\n",
    "                    self.scaler.step(self.wav2vec_optimizer)\n",
    "                    self.scaler.step(self.model_optimizer)\n",
    "                self.scaler.update()\n",
    "                self.optimizer_step += 1\n",
    "        else:\n",
    "            with self.no_sync():\n",
    "                outputs = self.compute_forward(batch, sb.Stage.TRAIN)\n",
    "            loss = self.compute_objectives(outputs, batch, sb.Stage.TRAIN)\n",
    "            (loss / self.grad_accumulation_factor).backward()\n",
    "            if should_step:\n",
    "                if self.check_gradients(loss):\n",
    "                    self.wav2vec_optimizer.step()\n",
    "                    self.model_optimizer.step()\n",
    "                self.wav2vec_optimizer.zero_grad()\n",
    "                self.model_optimizer.zero_grad()\n",
    "                self.optimizer_step += 1\n",
    "\n",
    "        return loss.detach().cpu()\n",
    "\n",
    "    def on_stage_start(self, stage, epoch):\n",
    "        \"\"\"Gets called at the beginning of each epoch\"\"\"\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.cer_metric = self.hparams.cer_computer()\n",
    "            self.wer_metric = self.hparams.error_rate_computer()\n",
    "\n",
    "    def on_stage_end(self, stage, stage_loss, epoch):\n",
    "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
    "        stage_stats = {\"loss\": stage_loss}\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            self.train_stats = stage_stats\n",
    "        else:\n",
    "            stage_stats[\"CER\"] = self.cer_metric.summarize(\"error_rate\")\n",
    "            stage_stats[\"WER\"] = self.wer_metric.summarize(\"error_rate\")\n",
    "\n",
    "        if stage == sb.Stage.VALID:\n",
    "            old_lr_model, new_lr_model = self.hparams.lr_annealing_model(\n",
    "                stage_stats[\"loss\"]\n",
    "            )\n",
    "            old_lr_wav2vec, new_lr_wav2vec = self.hparams.lr_annealing_wav2vec(\n",
    "                stage_stats[\"loss\"]\n",
    "            )\n",
    "            sb.nnet.schedulers.update_learning_rate(\n",
    "                self.model_optimizer, new_lr_model\n",
    "            )\n",
    "            sb.nnet.schedulers.update_learning_rate(\n",
    "                self.wav2vec_optimizer, new_lr_wav2vec\n",
    "            )\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                stats_meta={\n",
    "                    \"epoch\": epoch,\n",
    "                    \"lr_model\": old_lr_model,\n",
    "                    \"lr_wav2vec\": old_lr_wav2vec,\n",
    "                },\n",
    "                train_stats=self.train_stats,\n",
    "                valid_stats=stage_stats,\n",
    "            )\n",
    "            self.checkpointer.save_and_keep_only(\n",
    "                meta={\"WER\": stage_stats[\"WER\"]}, min_keys=[\"WER\"],\n",
    "            )\n",
    "        elif stage == sb.Stage.TEST:\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
    "                test_stats=stage_stats,\n",
    "            )\n",
    "            if if_main_process():\n",
    "                with open(self.hparams.test_wer_file, \"w\") as w:\n",
    "                    self.wer_metric.write_stats(w)\n",
    "\n",
    "    def init_optimizers(self):\n",
    "        \"Initializes the wav2vec2 optimizer and model optimizer\"\n",
    "        if hasattr(self.modules, \"extractor\"):\n",
    "            self.wav2vec_optimizer = self.hparams.wav2vec_opt_class(\n",
    "                self.modules.encoder_wrapper.parameters()\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.wav2vec_optimizer = self.hparams.wav2vec_opt_class(\n",
    "                self.modules.wav2vec2.parameters()\n",
    "            )\n",
    "\n",
    "        self.model_optimizer = self.hparams.model_opt_class(\n",
    "            self.hparams.model.parameters()\n",
    "        )\n",
    "\n",
    "        if self.checkpointer is not None:\n",
    "            self.checkpointer.add_recoverable(\n",
    "                \"wav2vec_opt\", self.wav2vec_optimizer\n",
    "            )\n",
    "            self.checkpointer.add_recoverable(\"modelopt\", self.model_optimizer)\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        self.wav2vec_optimizer.zero_grad(set_to_none)\n",
    "        self.model_optimizer.zero_grad(set_to_none)\n",
    "\n",
    "\n",
    "def dataio_prepare(hparams):\n",
    "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
    "    It also defines the data processing pipeline through user-defined functions.\"\"\"\n",
    "    data_folder = hparams[\"data_folder\"]\n",
    "\n",
    "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
    "        csv_path=hparams[\"train_csv\"], replacements={\"data_root\": data_folder},\n",
    "    )\n",
    "\n",
    "    if hparams[\"sorting\"] == \"ascending\":\n",
    "        train_data = train_data.filtered_sorted(sort_key=\"duration\")\n",
    "        hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "    elif hparams[\"sorting\"] == \"descending\":\n",
    "        train_data = train_data.filtered_sorted(\n",
    "            sort_key=\"duration\", reverse=True\n",
    "        )\n",
    "\n",
    "        hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "    elif hparams[\"sorting\"] == \"random\":\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            \"sorting must be random, ascending or descending\"\n",
    "        )\n",
    "\n",
    "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
    "        csv_path=hparams[\"valid_csv\"], replacements={\"data_root\": data_folder},\n",
    "    )\n",
    "    valid_data = valid_data.filtered_sorted(sort_key=\"duration\")\n",
    "\n",
    "    test_datasets = {}\n",
    "    for csv_file in hparams[\"test_csv\"]:\n",
    "        name = Path(csv_file).stem\n",
    "        test_datasets[name] = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
    "            csv_path=csv_file, replacements={\"data_root\": data_folder}\n",
    "        )\n",
    "        test_datasets[name] = test_datasets[name].filtered_sorted(\n",
    "            sort_key=\"duration\"\n",
    "        )\n",
    "\n",
    "    datasets = [train_data, valid_data] + [i for k, i in test_datasets.items()]\n",
    "    \n",
    "    @sb.utils.data_pipeline.takes(\"wav\")\n",
    "    @sb.utils.data_pipeline.provides(\"sig\")\n",
    "    def audio_pipeline(wav):\n",
    "        sig = sb.dataio.dataio.read_audio(wav)\n",
    "        return sig\n",
    "\n",
    "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
    "    label_encoder = sb.dataio.encoder.CTCTextEncoder()\n",
    "\n",
    "    @sb.utils.data_pipeline.takes(\"wrd\")\n",
    "    @sb.utils.data_pipeline.provides(\n",
    "        \"wrd\", \"char_list\", \"tokens_list\", \"tokens\"\n",
    "    )\n",
    "    def text_pipeline(wrd):\n",
    "        yield wrd\n",
    "        char_list = list(wrd)\n",
    "        yield char_list\n",
    "        tokens_list = label_encoder.encode_sequence(char_list)\n",
    "        yield tokens_list\n",
    "        tokens = torch.LongTensor(tokens_list)\n",
    "        yield tokens\n",
    "\n",
    "    sb.dataio.dataset.add_dynamic_item(datasets, text_pipeline)\n",
    "\n",
    "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
    "    special_labels = {\n",
    "        \"blank_label\": hparams[\"blank_index\"],\n",
    "    }\n",
    "    label_encoder.load_or_create(\n",
    "        path=lab_enc_file,\n",
    "        from_didatasets=[train_data],\n",
    "        output_key=\"char_list\",\n",
    "        special_labels=special_labels,\n",
    "        sequence_input=True,\n",
    "    )\n",
    "\n",
    "    sb.dataio.dataset.set_output_keys(\n",
    "        datasets, [\"id\", \"sig\", \"wrd\", \"char_list\", \"tokens\"],\n",
    "    )\n",
    "\n",
    "    return train_data, valid_data, test_datasets, label_encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44db9015",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed7b444b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad74a8ec35642c5b5609359cc7ee2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab491874fd98496482beb22fd890c576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea1ff5a52ac42669d7a511a31d9e51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration LibriSpeech-49d9aec4ade7f44e\n",
      "Reusing dataset text (/home/kmb85/.cache/huggingface/datasets/text/LibriSpeech-49d9aec4ade7f44e/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1871759fde4b6bbd83010dc29101d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "data = load_dataset(\"/home/kmb85/SpeechBrainQuant/LibriSpeech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72159ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_data import parse_to_json\n",
    "import json\n",
    "import os\n",
    "\n",
    "parse_to_json(\"./LibriSpeech/dev-clean\")\n",
    "os.rename('data.json', 'dev-clean.json')\n",
    "with open('/home/kmb85/SpeechBrainQuant/dev-clean.json', 'r') as file:\n",
    "    dev_clean_data = json.load(file)\n",
    "\n",
    "parse_to_json(\"./LibriSpeech/test-clean\")\n",
    "os.rename('data.json', 'test-clean.json')\n",
    "with open('/home/kmb85/SpeechBrainQuant/test-clean.json', 'r') as file:\n",
    "    test_clean_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f474fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_clean_dict = {}\n",
    "for key, value in dev_clean_data.items():\n",
    "    new_key = key.replace('-', '')\n",
    "    dev_clean_dict[new_key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1dd7a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean_dict = {}\n",
    "for key, value in test_clean_data.items():\n",
    "    new_key = key.replace('-', '')\n",
    "    test_clean_dict[new_key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b83c09d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import resampy\n",
    "\n",
    "def flac_to_array_with_sampling_rate(file_path, target_sr=16000):\n",
    "    data, sr = sf.read(file_path, dtype='float32')\n",
    "\n",
    "    if sr != target_sr:\n",
    "        data = resampy.resample(data, sr, target_sr)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "947b0ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"]'\n",
    "\n",
    "def remove_special_characters_dev(batch):\n",
    "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower() + \" \"\n",
    "    trimmed_string = batch[\"text\"].strip()\n",
    "    words = trimmed_string.split()\n",
    "    new_text = ' '.join(words[1:])\n",
    "    batch[\"text\"] = new_text\n",
    "    batch[\"input_values\"] = flac_to_array_with_sampling_rate(dev_clean_dict[words[0]]['file_path'])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1a8c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['validation'] = data['validation'].map(remove_special_characters_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da82949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"]'\n",
    "\n",
    "def remove_special_characters_test(batch):\n",
    "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower() + \" \"\n",
    "    trimmed_string = batch[\"text\"].strip()\n",
    "    words = trimmed_string.split()\n",
    "    new_text = ' '.join(words[1:])\n",
    "    batch[\"text\"] = new_text\n",
    "    batch[\"input_values\"] = flac_to_array_with_sampling_rate(test_clean_dict[words[0]]['file_path'])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d7b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['test'] = data['test'].map(remove_special_characters_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c187860",
   "metadata": {},
   "source": [
    "## Base Model Traing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10f3ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_file = 'hparams/train_hf_wav2vec_base.yaml'\n",
    "run_opts = {\"device\":\"cuda\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee711739",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.utils.distributed.ddp_init_group(run_opts)\n",
    "\n",
    "with open(hparams_file) as fin:\n",
    "    hparams = load_hyperpyyaml(fin, '')\n",
    "\n",
    "sb.create_experiment_directory(\n",
    "    experiment_directory=hparams[\"output_folder\"],\n",
    "    hyperparams_to_save=hparams_file,\n",
    "    overrides='',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad5954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from librispeech_prepare import prepare_librispeech\n",
    "\n",
    "run_on_main(\n",
    "    prepare_librispeech,\n",
    "    kwargs={\n",
    "        \"data_folder\": hparams[\"data_folder\"],\n",
    "        \"tr_splits\": hparams[\"train_splits\"],\n",
    "        \"dev_splits\": hparams[\"dev_splits\"],\n",
    "        \"te_splits\": hparams[\"test_splits\"],\n",
    "        \"save_folder\": hparams[\"output_folder\"],\n",
    "        \"merge_lst\": hparams[\"train_splits\"],\n",
    "        \"merge_name\": \"train.csv\",\n",
    "        \"skip_prep\": hparams[\"skip_prep\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8cf4a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.dataio.encoder - Moving label 'T' from index 0, because '<blank>' was put at its place.\n",
      "speechbrain.dataio.encoder - Load called, but CTCTextEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_datasets, label_encoder = dataio_prepare(\n",
    "    hparams\n",
    ")\n",
    "\n",
    "if hasattr(hparams, \"use_language_modelling\"):\n",
    "    if hparams[\"use_language_modelling\"]:\n",
    "        try:\n",
    "            from pyctcdecode import build_ctcdecoder\n",
    "        except ImportError:\n",
    "            err_msg = \"Optional dependencies must be installed to use pyctcdecode.\\n\"\n",
    "            err_msg += \"Install using `pip install kenlm pyctcdecode`.\\n\"\n",
    "            raise ImportError(err_msg)\n",
    "\n",
    "        ind2lab = label_encoder.ind2lab\n",
    "        labels = [ind2lab[x] for x in range(len(ind2lab))]\n",
    "        labels = [\"\"] + labels[\n",
    "            1:\n",
    "        ]  \n",
    "        decoder = build_ctcdecoder(\n",
    "            labels,\n",
    "            kenlm_model_path=hparams[\"ngram_lm_path\"],  # .arpa or .bin\n",
    "            alpha=0.5,  # Default by KenLM\n",
    "            beta=1.0,  # Default by KenLM\n",
    "        )\n",
    "else:\n",
    "    hparams[\"use_language_modelling\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a686a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.core - Info: auto_mix_prec arg from hparam file is used\n",
      "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
      "speechbrain.core - 2.1M trainable parameters in ASR\n"
     ]
    }
   ],
   "source": [
    "asr_brain_base = ASR(\n",
    "    modules=hparams[\"modules\"],\n",
    "    hparams=hparams,\n",
    "    run_opts=run_opts,\n",
    "    checkpointer=hparams[\"checkpointer\"],\n",
    ")\n",
    "\n",
    "if \"pretrainer\" in hparams.keys():\n",
    "    run_on_main(hparams[\"pretrainer\"].collect_files)\n",
    "    hparams[\"pretrainer\"].load_collected('cuda')\n",
    "\n",
    "asr_brain_base.tokenizer = label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efb4534",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "asr_brain_base.fit(\n",
    "    asr_brain_base.hparams.epoch_counter,\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_loader_kwargs=hparams[\"train_dataloader_opts\"],\n",
    "    valid_loader_kwargs=hparams[\"valid_dataloader_opts\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f356297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "if not os.path.exists(hparams[\"output_wer_folder\"]):\n",
    "    os.makedirs(hparams[\"output_wer_folder\"])\n",
    "\n",
    "for k in test_datasets.keys():\n",
    "    asr_brain_base.hparams.test_wer_file = os.path.join(\n",
    "        hparams[\"output_wer_folder\"], f\"wer_{k}.txt\"\n",
    "    )\n",
    "    asr_brain_base.evaluate(\n",
    "        test_datasets[k],\n",
    "        test_loader_kwargs=hparams[\"test_dataloader_opts\"],\n",
    "        min_key=\"WER\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8456bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(asr_brain_base.modules.wav2vec2.model,\n",
    "     torch.tensor(data['validation'][:1]['input_values']).cuda(),\n",
    "     f=\"asr-wav2vec2-librispeech-finetuned-base.onnx\",\n",
    "     export_params=True,\n",
    "     opset_version=17,\n",
    "     do_constant_folding=True,\n",
    "     input_names = ['modelInput'],\n",
    "     output_names = ['modelOutput'],\n",
    "     dynamic_axes={'modelInput' : {0 : 'batch_size', 1: 'batch_size'},\n",
    "     'modelOutput' : {0 : 'batch_size'}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38745cd0",
   "metadata": {},
   "source": [
    "## Quantization-Aware Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8f0d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_file = 'hparams/train_hf_wav2vec_quantized.yaml'\n",
    "run_opts = {\"device\":\"cuda\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2093688e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3309a1050f7243af81226aa848c3c746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/158 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758bdb6f1925409bb44d4758580e36ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c16158457949fcaeee349e0e79b71a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kmb85/.local/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-large-960h-lv60-self were not used when initializing Wav2Vec2Model: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 is frozen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: results/train_wav2vec2_char/1987\n"
     ]
    }
   ],
   "source": [
    "sb.utils.distributed.ddp_init_group(run_opts)\n",
    "\n",
    "with open(hparams_file) as fin:\n",
    "    hparams = load_hyperpyyaml(fin, '')\n",
    "\n",
    "sb.create_experiment_directory(\n",
    "    experiment_directory=hparams[\"output_folder\"],\n",
    "    hyperparams_to_save=hparams_file,\n",
    "    overrides='',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad812fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from librispeech_prepare import prepare_librispeech\n",
    "\n",
    "run_on_main(\n",
    "    prepare_librispeech,\n",
    "    kwargs={\n",
    "        \"data_folder\": hparams[\"data_folder\"],\n",
    "        \"tr_splits\": hparams[\"train_splits\"],\n",
    "        \"dev_splits\": hparams[\"dev_splits\"],\n",
    "        \"te_splits\": hparams[\"test_splits\"],\n",
    "        \"save_folder\": hparams[\"output_folder\"],\n",
    "        \"merge_lst\": hparams[\"train_splits\"],\n",
    "        \"merge_name\": \"train.csv\",\n",
    "        \"skip_prep\": hparams[\"skip_prep\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6955b034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.dataio.encoder - Load called, but CTCTextEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_datasets, label_encoder = dataio_prepare(\n",
    "    hparams\n",
    ")\n",
    "\n",
    "if hasattr(hparams, \"use_language_modelling\"):\n",
    "    if hparams[\"use_language_modelling\"]:\n",
    "        try:\n",
    "            from pyctcdecode import build_ctcdecoder\n",
    "        except ImportError:\n",
    "            err_msg = \"Optional dependencies must be installed to use pyctcdecode.\\n\"\n",
    "            err_msg += \"Install using `pip install kenlm pyctcdecode`.\\n\"\n",
    "            raise ImportError(err_msg)\n",
    "\n",
    "        ind2lab = label_encoder.ind2lab\n",
    "        labels = [ind2lab[x] for x in range(len(ind2lab))]\n",
    "        labels = [\"\"] + labels[\n",
    "            1:\n",
    "        ]  \n",
    "        decoder = build_ctcdecoder(\n",
    "            labels,\n",
    "            kenlm_model_path=hparams[\"ngram_lm_path\"],  # .arpa or .bin\n",
    "            alpha=0.5,  # Default by KenLM\n",
    "            beta=1.0,  # Default by KenLM\n",
    "        )\n",
    "else:\n",
    "    hparams[\"use_language_modelling\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "419fcfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.core - Info: auto_mix_prec arg from hparam file is used\n",
      "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
      "speechbrain.core - 2.1M trainable parameters in ASR\n"
     ]
    }
   ],
   "source": [
    "asr_brain_quantized = ASR(\n",
    "    modules=hparams[\"modules\"],\n",
    "    hparams=hparams,\n",
    "    run_opts=run_opts,\n",
    "    checkpointer=hparams[\"checkpointer\"],\n",
    ")\n",
    "\n",
    "if \"pretrainer\" in hparams.keys():\n",
    "    run_on_main(hparams[\"pretrainer\"].collect_files)\n",
    "    hparams[\"pretrainer\"].load_collected('cuda')\n",
    "\n",
    "asr_brain_quantized.tokenizer = label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a719757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0eff129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/kmb85/rds/hpc-work/huggingface'\n",
    "os.environ['HF_HOME']='/home/kmb85/rds/hpc-work/huggingface'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a83387f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import quant_modules\n",
    "from pytorch_quantization.tensor_quant import QuantDescriptor\n",
    "from pytorch_quantization import calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a6acf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_desc_input = QuantDescriptor(calib_method='max', num_bits=8, fake_quant=True)\n",
    "quant_nn.QuantConv1d.set_default_quant_desc_input(quant_desc_input)\n",
    "quant_nn.QuantLinear.set_default_quant_desc_input(quant_desc_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee72273f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a50d56ff0d40ee887e887e1ba3e14d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hyperparams.yaml:   0%|          | 0.00/1.78k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f333b8eeaa7f4e36a51760d34d44fcfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-large-960h-lv60-self were not used when initializing Wav2Vec2Model: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 is frozen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0108 14:06:04.263104 140341998555136 huggingface_wav2vec.py:146] speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 is frozen.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f1ba8193a746f988646cf91969635f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wav2vec2.ckpt:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d8f27ff97f4ef39aa3a8299744f795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "asr.ckpt:   0%|          | 0.00/8.53M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3e5efed8014a1a9f18d3583b4da7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.ckpt:   0%|          | 0.00/426 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from speechbrain.pretrained import EncoderASR\n",
    "\n",
    "quant_modules.initialize()\n",
    "model_full_quantized = EncoderASR.from_hparams(source=\"speechbrain/asr-wav2vec2-librispeech\", run_opts={\"device\":\"cuda\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3bdb0e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full_quantized.mods.encoder.wav2vec2.model.feature_extractor.conv_layers[0].conv._weight_quantizer.enable_calib()\n",
    "model_full_quantized.mods.encoder.wav2vec2.model.feature_extractor.conv_layers[0].conv._weight_quantizer.disable_quant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8ec83e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full_quantized = model_full_quantized.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99acb40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    for i in range(2000):\n",
    "            input_value = torch.tensor(data[\"validation\"][i:i+1][\"input_values\"], device=\"cuda\")\n",
    "            _ = model_full_quantized.mods.encoder.wav2vec2.model(input_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bcc152f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl - Load calibrated amax, shape=torch.Size([512, 1, 1]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0108 14:13:53.678715 140341998555136 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([512, 1, 1]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl - Call .cuda() if running on GPU after loading calibrated amax.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0108 14:13:53.681514 140341998555136 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl - Disable MaxCalibrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0108 14:13:53.682647 140341998555136 tensor_quantizer.py:174] Disable MaxCalibrator\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncoderASR(\n",
       "  (mods): ModuleDict(\n",
       "    (encoder): LengthsCapableSequential(\n",
       "      (wav2vec2): HuggingFaceWav2Vec2(\n",
       "        (model): Wav2Vec2Model(\n",
       "          (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "            (conv_layers): ModuleList(\n",
       "              (0): Wav2Vec2LayerNormConvLayer(\n",
       "                (conv): QuantConv1d(\n",
       "                  1, 512, kernel_size=(10,), stride=(5,)\n",
       "                  (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                  (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0071, 0.6079](512) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                )\n",
       "                (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "                (conv): QuantConv1d(\n",
       "                  512, 512, kernel_size=(3,), stride=(2,)\n",
       "                  (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                  (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                )\n",
       "                (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "                (conv): QuantConv1d(\n",
       "                  512, 512, kernel_size=(2,), stride=(2,)\n",
       "                  (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                  (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                )\n",
       "                (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (feature_projection): Wav2Vec2FeatureProjection(\n",
       "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (projection): QuantLinear(\n",
       "              in_features=512, out_features=1024, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "            (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "              (conv): QuantConv1d(\n",
       "                1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "                (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "              )\n",
       "              (padding): Wav2Vec2SamePadLayer()\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layers): ModuleList(\n",
       "              (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "                (attention): Wav2Vec2Attention(\n",
       "                  (k_proj): QuantLinear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                    (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                  )\n",
       "                  (v_proj): QuantLinear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                    (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                  )\n",
       "                  (q_proj): QuantLinear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                    (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                  )\n",
       "                  (out_proj): QuantLinear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                    (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (feed_forward): Wav2Vec2FeedForward(\n",
       "                  (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (intermediate_dense): QuantLinear(\n",
       "                    in_features=1024, out_features=4096, bias=True\n",
       "                    (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                    (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (output_dense): QuantLinear(\n",
       "                    in_features=4096, out_features=1024, bias=True\n",
       "                    (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                    (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                  )\n",
       "                  (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (enc): VanillaNN(\n",
       "        (linear): Linear(\n",
       "          (w): QuantLinear(\n",
       "            in_features=1024, out_features=1024, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "        )\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (linear_0): Linear(\n",
       "          (w): QuantLinear(\n",
       "            in_features=1024, out_features=1024, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "        )\n",
       "        (act_0): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (ctc_lin): Linear(\n",
       "        (w): QuantLinear(\n",
       "          in_features=1024, out_features=31, bias=True\n",
       "          (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finalize calibration\n",
    "\n",
    "model_full_quantized.mods.encoder.wav2vec2.model.feature_extractor.conv_layers[0].conv._weight_quantizer.load_calib_amax()\n",
    "model_full_quantized.mods.encoder.wav2vec2.model.feature_extractor.conv_layers[0].conv._weight_quantizer.disable_calib()\n",
    "model_full_quantized.mods.encoder.wav2vec2.model.feature_extractor.conv_layers[0].conv._weight_quantizer.enable_quant()\n",
    "\n",
    "model_full_quantized.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1b077bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_brain_quantized.modules.wav2vec2.model.feature_extractor.conv_layers[0].conv = model_full_quantized.mods.encoder.wav2vec2.model.feature_extractor.conv_layers[0].conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f3a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_brain_quantized.fit(\n",
    "    asr_brain_quantized.hparams.epoch_counter,\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_loader_kwargs=hparams[\"train_dataloader_opts\"],\n",
    "    valid_loader_kwargs=hparams[\"valid_dataloader_opts\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e26ae65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - Epoch loaded: 2 - test loss: 4.83e-02, test CER: 6.08e-01, test WER: 2.44\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "if not os.path.exists(hparams[\"output_wer_folder\"]):\n",
    "    os.makedirs(hparams[\"output_wer_folder\"])\n",
    "\n",
    "for k in test_datasets.keys():\n",
    "    asr_brain_quantized.hparams.test_wer_file = os.path.join(\n",
    "        hparams[\"output_wer_folder\"], f\"wer_{k}.txt\"\n",
    "    )\n",
    "    asr_brain_quantized.evaluate(\n",
    "        test_datasets[k],\n",
    "        test_loader_kwargs=hparams[\"test_dataloader_opts\"],\n",
    "        min_key=\"WER\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ba611da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kmb85/.local/lib/python3.10/site-packages/pytorch_quantization/tensor_quant.py:257: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if amax.numel() == 1:\n",
      "/home/kmb85/.local/lib/python3.10/site-packages/pytorch_quantization/tensor_quant.py:260: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  axis = amax.shape.index(amax.numel())\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(asr_brain_quantized.modules.wav2vec2.model,\n",
    "     torch.tensor(data['validation'][:1]['input_values']).cuda(),\n",
    "     f=\"asr-wav2vec2-librispeech-qat.onnx\",\n",
    "     export_params=True,\n",
    "     opset_version=17,\n",
    "     do_constant_folding=True,\n",
    "     input_names = ['modelInput'],\n",
    "     output_names = ['modelOutput'],\n",
    "     dynamic_axes={'modelInput' : {0 : 'batch_size', 1: 'batch_size'},\n",
    "     'modelOutput' : {0 : 'batch_size'}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fca267",
   "metadata": {},
   "source": [
    "## Base vs Quantized Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0227e556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAGDCAYAAABjkcdfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAjRElEQVR4nO3debglVXm28fsBGsEwKbQy0w6oAQSFlgioIUETGZRPJQpxNgb1wwGjSdT4BTUO0RgUwYgYUHEAxyAGjLMMytTdMgiIIEJoQW0QaBBkaN7vj6ojm8MZNn26TnXvvn/Xta+z96raVe8e+vRzVq1alapCkiRJs2uNvguQJElaHRnCJEmSemAIkyRJ6oEhTJIkqQeGMEmSpB4YwiRJknpgCJPUiyQXJ9mz7zpGSZJPJXl333UMK8nbkvznCt7mnkkWr8htSl0xhEnLKclbk5w6ru3ySdoObO9Xkt8luXXg9g/tsnckuattuynJj5LsNk0NmyU5Nsl1SW5J8tMk70zyRyv69a5oVbV9Vf2gr/0neVmSZQOfw5VJXtNXPbMhyZZJPpfkhvZ7eG6SfWZp3/cLR1X13qp65WzsX1oZGcKk5Xc6sEeSNQGSbArMAXYe1/bodt0xO1XVegO3Dwws+0JVrQdsAnwf+NJkO0/yUOAsYF1gt6paH3gGsBHwqBX0Gle4JGv1XcOAs8Y+B+AA4ANJnth3UV1ovy9nAncC29N8xz4EnJjk//RYmrTaMoRJy+88mtD1hPbx02iC02Xj2n5eVdc+kA1X1d3A54AtksydZLW/A24BXlRVV7XPu6aq3lBVFwIk2T3JeUlubn/uPvbkJD9I8u62x+3WJF9PsnHbU7K0XX/ewPqV5PVtj9H1Sf4tyRrtskcl+V7bw3J9u42NBp57VZJ/THIh8Lska7VtT2+X75pkQbvfXyc5fOC5z24PXd7U1vzH47b75iQXtq/xC0nWeSDv9cB7vgi4FBjc/peS/Krd9ulJth9Ytk+SS9oeyF8mefPAsv2SnD/Qo7njZPtNckSSa9rXvjDJUweWvSPJF5Mc3+7n4iTzB5Y/McmidtkXgKle+xuBW4G/qapfVdXtVXUC8B7g8DTmtZ/zH4Jy+56/sr0/zOd8v88jTc/sN4DNc2/P4+bt6/ts+9yjct8e4ruTvKNdtnmSryRZkuQXSV4/sM910xyGvTHJJcCTpngPpJWKIUxaTlV1J3AOTdCi/XkGTW/DYNvp93/21JKsDbwEuAG4cZLVng58tarumWQbDwVOAT4CbAwcDpySZOOB1Q4EXgxsQdN7dhbwSeChNIHksHGbfQ4wH9gZ2B94xdjugPcBm9OEmK2Ad4x77kHAvsBGbcgcdARwRFVt0NbxxfY1PAY4ATgUmAucCny9fX/GPB94JvAIYEfgZQPvwU1JnjLR+zNekicBjwEWDDR/A9gWeBiwiCYYjzkWeFXbA7kD8L12OzsDxwGvonnfPw6cnORBk+z6PJrQ/lDg88CXxgXJZwMn0vRwngwc1e5nbeAk4DPtc78EPG+Kl/gM4CsTfF++SPPePXqK544Z5nO+3+dRVb8D9gauHegBvs8fJlX12oFeyafQfO+/1gb9rwMX0HxP9wIOTfKX7VMPo/nOPAr4S+ClQ7wOaaVgCJNm5jTuDVxPpQlhZ4xrO23ccxa14WDs9pcDy56f5CbgduBvgQMmCCxjNgaum6K2fYHLq+ozVXV32+vxU+BZA+t8sqp+XlU30wSOn1fVd9p9fgkYf2ju/VX126r6X+DDNMGKqrqiqr5dVXdU1RKawPen4577kban7vYJar0LeHSSTarq1qo6u21/AXBKu+27gA/SHH7dfeC5H6mqa6vqtzT/WT9hbEFVbVRVZ07xHj25/QxuBc6lCTSXDzz/uKq6paruoAkbOyXZcKDm7ZJsUFU3tj1p0HxuH6+qc6pqWVV9GrgDePJEBVTVZ6vqhvYz+nfgQcBjB1Y5s6pOraplbX07jdVO0xP74aq6q6q+TBPoJrMJE39fxtom63EdrHXYz3nCz2MYaXp+TwJeV1U/punZmltV76qqO6vqSuATNH9AQBP63tN+L6+h+aNDWiUYwqSZOR14SpKH0PxHcTnwI2D3tm0H7t8TtnMbDsZu3xxY9sWq2gh4OPATYJcp9n0DsNkUyzcHrh7XdjVNb8KYXw/cv32Cx+uNe/4147a1OUCShyU5sT0stxT4LM1/+pM9d7y/oemF+mmaw6D7TfQa2l6ca8a9hl8N3L9tgpqncnb7GawHbEozVuq97WtaM8m/Jvl5+5quap8z9rqeB+wDXJ3ktNx7EsU2wJsGgzZNj9HmExWQ5E1JLm0P390EbMh937vxr2+d9nDh5sAvq6oGlo//vAddz8Tfl7G2JVM8d6zWYT7n5f48kswBvgx8vqpObJu3oTmMOfh+vo3m3wg078P476W0SjCESTNzFs1/mgcDPwSoqqXAtW3btVX1iwe60aq6nuZw1juSTBa0vgM8pz1cM5Fraf4DG7Q18MsHWs+ArcZta+yQ0vuAAnZsDym+iObQ1aBiElV1eVUdRHPY7/3Al9txRPd5DUnS1jCT1zBZDb8GvsK9PYV/TXPI9ek0n/G8sTLa9c+rqv3bmk+iPYRKEwjeMy5oP7jtibyPdvzXP9L05jykDeA3c//3biLX0YwZHFx36ynW/w7wvAm+L88HFgM/B37Xtj14YPmmA/eH+ZwnM+nnP+BImnGObx9ouwb4xbj3c/2qGjur8zru/72UVgmGMGkG2kNrC2gGyZ8xsOjMtu0Bjwcb2PZPgW8C/zDJKocDGwCfTrINQJItkhyeZiD4qcBjkvx1moHwLwC2A/57eWsC/j7JQ5JsBbwB+ELbvj7NoO+bkmwB/P0D2WiSFyWZ2/Z03dQ2L6MJNvsm2avtJXkTzaG9H83gNUxWw8Y0Y94ubpvWb/d1A00oee/AumsneWGSDdvDpEvbeqE5VPbqJH+Sxh8l2TfJ+hPsdn3gbppeqLWS/DPNZzqMs9rnvr79fJ8L7DrF+h9qt31skk3bAfMHAf8POKyq7mkPMf4SeFHbE/gK7num7Uw+518DGw8czr2PJK+iObT51+PGrZ0LLE1zYse6bV07tGP4oPmOvLX9Xm4JvO4B1CT1yhAmzdxpNL0hg2OPzmjbJgphF+S+Z4F9eIpt/xtwcJKHjV/QjrnZnWZs0jlJbgG+S9OTckVV3QDsRxNcbqAJc/u1vWzL62vAQuB8mkH/x7bt76QZrH9z2/7VB7jdZwIXt2OzjgAOrKrfV9VlNL0tR9IcTnsW8Kz2pIhpte/vU6dYZbexz4HmRIQl3Puf+PE0h7Z+CVwCnD3uuS8GrmoPy726rZOqWkAzLuwomsHlVzBwssA436QZi/ezdl+/Z+rDtn/QvgfPbbd9I834uUnf9/b78BSaMygvoQlTxwOHVNVxA6v+LU24uoHm8Oxg4F3uz7n9o+IE4Mr2sOL4w7MHAY8Erh34t/G2dizcs2jGlv2C5nvwnzS9k2M1Xd0u+xbNuDlplZD7DieQpIklKWDbqrqi71o0c0k2oDmE/l9V9c991yOtjuwJk6TVUDt2cR9gWZpJhSXNMnvCJA3FnjBJWrEMYZIkST3wcKQkSVIPDGGSJEk9WGv6VVYum2yySc2bN6/vMiRJkqa1cOHC66tqwsuCrXIhbN68eSxYsGD6FSVJknqWZNJLaXk4UpIkqQeGMEmSpB4YwiRJknpgCJMkSeqBIUySJKkHhjBJkqQeGMIkSZJ6YAiTJEnqgSFMkiSpB4YwSZKkHhjCJEmSemAIkyRJ6oEhTJIkqQeGMEmSpB4YwiRJknpgCJMkSeqBIUySJKkHnYWwJFsl+X6SS5NcnOQNE6yzZ5Kbk5zf3v65q3okSZJWJmt1uO27gTdV1aIk6wMLk3y7qi4Zt94ZVbVfh3VIkiStdDrrCauq66pqUXv/FuBSYIuu9idJkrQqmZUxYUnmAU8Ezplg8W5JLkjyjSTbT/L8g5MsSLJgyZIlXZYqSZI0KzoPYUnWA74CHFpVS8ctXgRsU1U7AUcCJ020jao6pqrmV9X8uXPndlqvJEnSbOg0hCWZQxPAPldVXx2/vKqWVtWt7f1TgTlJNumyJkmSpJVBl2dHBjgWuLSqDp9knU3b9Uiya1vPDV3VJEmStLLo8uzIPYAXAxclOb9texuwNUBVHQ0cALwmyd3A7cCBVVUd1iRJkrRS6CyEVdWZQKZZ5yjgqK5qkCRJWlk5Y74kSVIPujwcKUmaxLy3nNJ3CdJq76p/3bfX/dsTJkmS1ANDmCRJUg8MYZIkST0whEmSJPXAECZJktQDQ5gkSVIPDGGSJEk9MIRJkiT1wMlaJ+FEilK/+p5EUZK6Zk+YJElSDwxhkiRJPTCESZIk9cAQJkmS1ANDmCRJUg8MYZIkST0whEmSJPXAECZJktQDQ5gkSVIPDGGSJEk9MIRJkiT1wBAmSZLUA0OYJElSDwxhkiRJPTCESZIk9cAQJkmS1ANDmCRJUg8MYZIkST0whEmSJPXAECZJktQDQ5gkSVIPDGGSJEk9MIRJkiT1wBAmSZLUA0OYJElSDwxhkiRJPTCESZIk9cAQJkmS1ANDmCRJUg8MYZIkST0whEmSJPXAECZJktQDQ5gkSVIPDGGSJEk9MIRJkiT1wBAmSZLUA0OYJElSDwxhkiRJPTCESZIk9cAQJkmS1IPOQliSrZJ8P8mlSS5O8oYJ1kmSjyS5IsmFSXbuqh5JkqSVyVodbvtu4E1VtSjJ+sDCJN+uqksG1tkb2La9/QnwsfanJEnSSOusJ6yqrquqRe39W4BLgS3GrbY/cHw1zgY2SrJZVzVJkiStLGZlTFiSecATgXPGLdoCuGbg8WLuH9QkSZJGTuchLMl6wFeAQ6tq6fjFEzylJtjGwUkWJFmwZMmSLsqUJEmaVZ2GsCRzaALY56rqqxOsshjYauDxlsC141eqqmOqan5VzZ87d243xUqSJM2iLs+ODHAscGlVHT7JaicDL2nPknwycHNVXddVTZIkSSuLLs+O3AN4MXBRkvPbtrcBWwNU1dHAqcA+wBXAbcDLO6xHkiRppdFZCKuqM5l4zNfgOgUc0lUNkiRJKytnzJckSeqBIUySJKkHhjBJkqQeGMIkSZJ6YAiTJEnqgSFMkiSpB4YwSZKkHhjCJEmSemAIkyRJ6oEhTJIkqQeGMEmSpB4YwiRJknpgCJMkSeqBIUySJKkHhjBJkqQeGMIkSZJ6YAiTJEnqgSFMkiSpB4YwSZKkHhjCJEmSemAIkyRJ6oEhTJIkqQeGMEmSpB6sNeyKSR4CbA7cDlxVVfd0VpUkSdKImzKEJdkQOAQ4CFgbWAKsAzw8ydnAf1TV9zuvUpIkacRM1xP2ZeB44KlVddPggiS7AC9O8siqOraj+iRJkkbSlCGsqp4xxbKFwMIVXpEkSdJqYLkG5id5bJJPrOhiJEmSVhdThrAkOyb5VpKfJHl3kocn+QrwXeCS2SlRkiRp9EzXE/YJ4PPA82gG5S8CrgQeXVUf6rg2SZKkkTXdwPwHVdWn2vuXJXkz8JaqWtZtWZIkSaNtuhC2TpInAmkf3wrsmCQAVbWoy+IkSZJG1XQh7FfA4ZM8LuDPuyhKkiRp1E03RcWes1SHJEnSamW6syP/YeD+X41b9t6uipIkSRp1050deeDA/beOW/bMFVyLJEnSamO6EJZJ7k/0WJIkSUOaLoTVJPcneixJkqQhTXd25E5JltL0eq3b3qd9vE6nlUmSJI2w6c6OXHO2CpEkSVqdTHd25IeTHJBk89kqSJIkaXUw3eHIK4DnAh9sJ8n/EfDD9ucFVXVPt+VJkiSNpukORx4FHAWQZDNgD2B34I3Aw4ANui5QkiRpFE3XE0Z7ncjH04SvPYDtaHrIPtNtaZIkSaNryhCW5Ns0vV3nA2cD762qS2ehLkmSpJE23TxhV9LMB7Zte3t0kk06r0qSJGnETTcm7FUASTYAnkxzSPKQJHOBn1TVS7svUZIkafRMOyasdQdwG3B7e39LYO2uipIkSRp1080T9qEk5wDXAe8C1gc+Djy2qh4/C/VJkiSNpOl6wn4BfA74cVUtm4V6JEmSVgvTDcw/uaoWTBbA0tiyg7okSZJG2nQ9Yf+WZA3ga8BCYAnNhbsfDfwZsBdwGLC4yyIlSZJGzXRnR/5Vku2AFwKvADajGaB/KXAq8J6q+v1Ez01yHLAf8Juq2mGC5XvShLtftE1frap3Ld/LkCRJWrVMe3ZkVV0C/NNybPtTNJc8On6Kdc6oqv2WY9uSJEmrtOnGhAGQ5MFJ3p7kmPbxtkmmDE9VdTrw2xVQoyRJ0sgZKoQBnwTupJmsFZoxYO9eAfvfLckFSb6RZPsVsD1JkqRVwrAh7FFV9QHgLoCquh3IDPe9CNimqnYCjgROmmzFJAcnWZBkwZIlS2a4W0mSpP4NG8LuTLIuzXUkSfIompnzl1tVLa2qW9v7pwJzJrsuZVUdU1Xzq2r+3LlzZ7JbSZKklcKwly16B/A/wFZJPgfsAbx8JjtOsinw66qqJLvSBMIbZrJNSZKkVcVQIayqvpVkIc1FvAO8oaqun+o5SU4A9gQ2SbKYZj6xOe32jgYOAF6T5G6aa1IeWFW1vC9EkiRpVTJUCEvy3araCzhlgrYJVdVBU22zqo6imcJCkiRptTNlCEuyDvBgmt6sh3DvYPwNgM07rk2SJGlkTdcT9irgUJrAtZB7Q9hS4KPdlSVJkjTaprts0RHAEUleV1VHzlJNkiRJI2/YgflHJtkB2I7mAt5j7VNdkkiSJEmTGHZg/mE0ZzpuR3Ph7r2BM5n6upCSJEmaxLCTtR4A7AX8qqpeDuwEPKizqiRJkkbcsCHs9qq6B7g7yQbAb4BHdleWJEnSaBt2xvwFSTYCPkFzluStwLldFSVJkjTqpg1hSQK8r6puAo5O8j/ABlV1YdfFSZIkjappD0e2lxI6aeDxVQYwSZKkmRl2TNjZSZ7UaSWSJEmrkWHHhP0Z8KokVwO/o5k5v6pqx84qkyRJGmHDhrC9O61CkiRpNTPsjPlXd12IJEnS6mTYMWGSJElagQxhkiRJPZg2hCVZM8l3ZqMYSZKk1cUw84QtA25LsuEs1CNJkrRaGPbsyN8DFyX5Ns0UFQBU1es7qUqSJGnEDRvCTmlvkiRJWgGGnaLi00nWBh7TNl1WVXd1V5YkSdJoGyqEJdkT+DRwFc1s+VsleWlVnd5ZZZIkSSNs2MOR/w78RVVdBpDkMcAJwC5dFSZJkjTKhp0nbM5YAAOoqp8Bc7opSZIkafQN2xO2MMmxwGfaxy8EFnZTkiRJ0ugbNoS9GjgEeD3NmLDTgf/oqihJkqRRN20IS7IGsLCqdgAO774kSZKk0TfMjPn3ABck2XoW6pEkSVotDHs4cjPg4iTnct8Z85/dSVWSJEkjbtgQ9s5Oq5AkSVrNDDsm7KPtmDBJkiStAI4JkyRJ6oFjwiRJknrgmDBJkqQeDBXCquq0JNsA21bVd5I8GFiz29IkSZJG11DXjkzyt8CXgY+3TVsAJ3VUkyRJ0sgb9gLehwB7AEsBqupy4GFdFSVJkjTqhg1hd1TVnWMPkqwFVDclSZIkjb5hQ9hpSd4GrJvkGcCXgK93V5YkSdJoGzaEvQVYAlwEvAo4FXh7V0VJkiSNumHPjrwH+ER7kyRJ0gwN2xMmSZKkFcgQJkmS1ANDmCRJUg+mHBOW5OtMMRWF146UJElaPtMNzP9g+/O5wKbAZ9vHBwFXdVSTJEnSyJsyhFXVaQBJ/qWqnjaw6OtJTu+0MkmSpBE27JiwuUkeOfYgySOAud2UJEmSNPqGmicMOBT4QZIr28fzgIO7KEiSJGl1MG0IS7IGsCGwLfC4tvmnVXVHl4VJkiSNsmkPR7az5b+2qu6oqgvamwFMkiRpBoYdE/btJG9OslWSh47dOq1MkiRphA07JuwV7c9DBtoKeOQE6wKQ5DhgP+A3VbXDBMsDHAHsA9wGvKyqFg1ZjyRJ0ipt2At4P2I5tv0p4Cjg+EmW700zzmxb4E+Aj7U/JUmSRt5QISzJHOA1wNhcYT8APl5Vd032nKo6Pcm8KTa7P3B8VRVwdpKNkmxWVdcNVbkkSdIqbNgxYR8DdgH+o73t0rbNxBbANQOPF7dt95Pk4CQLkixYsmTJDHcrSZLUv2HHhD2pqnYaePy9JBfMcN+ZoG3C61RW1THAMQDz58+f9FqWkiRJq4phe8KWJXnU2IN29vxlM9z3YmCrgcdbAtfOcJuSJEmrhGF7wv4e+H47Y36AbYCXz3DfJwOvTXIizYD8mx0PJkmSVhdThrAkhwI/BE6jOYvxsTQhbNoZ85OcAOwJbJJkMXAYMAegqo4GTqWZnuIKmikqZhrqJEmSVhnT9YRtSTOX1+OAC4Ef0YSya4ApQ1hVHTTN8uK+845JkiStNqYMYVX1ZoAkawPzgd1pJm79RJKbqmq77kuUJEkaPcOOCVsX2IDmQt4b0gygv6iroiRJkkbddGPCjgG2B24BzqE5HHl4Vd04C7VJkiSNrOmmqNgaeBDwK+CXNNNK3NRxTZIkSSNvujFhz2wvtL09zXiwNwE7JPktcFZVHTYLNUqSJI2caceEtWcx/iTJTcDN7W0/YFeaaSckSZL0AE03Juz1ND1gewB30UxPcRZwHA7MlyRJWm7T9YTNA74MvNHZ7CVJklac6caE/d1sFSJJkrQ6GfYC3pIkSVqBDGGSJEk9MIRJkiT1wBAmSZLUA0OYJElSDwxhkiRJPTCESZIk9cAQJkmS1ANDmCRJUg8MYZIkST0whEmSJPXAECZJktQDQ5gkSVIPDGGSJEk9MIRJkiT1wBAmSZLUA0OYJElSDwxhkiRJPTCESZIk9cAQJkmS1ANDmCRJUg8MYZIkST0whEmSJPXAECZJktQDQ5gkSVIPDGGSJEk9MIRJkiT1wBAmSZLUA0OYJElSDwxhkiRJPTCESZIk9cAQJkmS1ANDmCRJUg8MYZIkST0whEmSJPXAECZJktQDQ5gkSVIPDGGSJEk9MIRJkiT1wBAmSZLUA0OYJElSDwxhkiRJPTCESZIk9aDTEJbkmUkuS3JFkrdMsHzPJDcnOb+9/XOX9UiSJK0s1upqw0nWBD4KPANYDJyX5OSqumTcqmdU1X5d1SFJkrQy6rInbFfgiqq6sqruBE4E9u9wf5IkSauMLkPYFsA1A48Xt23j7ZbkgiTfSLL9RBtKcnCSBUkWLFmypItaJUmSZlWXISwTtNW4x4uAbapqJ+BI4KSJNlRVx1TV/KqaP3fu3BVbpSRJUg+6DGGLga0GHm8JXDu4QlUtrapb2/unAnOSbNJhTZIkSSuFLkPYecC2SR6RZG3gQODkwRWSbJok7f1d23pu6LAmSZKklUJnZ0dW1d1JXgt8E1gTOK6qLk7y6nb50cABwGuS3A3cDhxYVeMPWUqSJI2czkIY/OEQ46nj2o4euH8UcFSXNUiSJK2MnDFfkiSpB4YwSZKkHhjCJEmSemAIkyRJ6oEhTJIkqQeGMEmSpB4YwiRJknpgCJMkSeqBIUySJKkHhjBJkqQeGMIkSZJ6YAiTJEnqgSFMkiSpB4YwSZKkHhjCJEmSemAIkyRJ6oEhTJIkqQeGMEmSpB4YwiRJknpgCJMkSeqBIUySJKkHhjBJkqQeGMIkSZJ6YAiTJEnqgSFMkiSpB4YwSZKkHhjCJEmSemAIkyRJ6oEhTJIkqQeGMEmSpB4YwiRJknpgCJMkSeqBIUySJKkHhjBJkqQeGMIkSZJ6YAiTJEnqgSFMkiSpB4YwSZKkHhjCJEmSemAIkyRJ6oEhTJIkqQeGMEmSpB4YwiRJknpgCJMkSeqBIUySJKkHhjBJkqQeGMIkSZJ6YAiTJEnqgSFMkiSpB4YwSZKkHhjCJEmSetBpCEvyzCSXJbkiyVsmWJ4kH2mXX5hk5y7rkSRJWll0FsKSrAl8FNgb2A44KMl241bbG9i2vR0MfKyreiRJklYmXfaE7QpcUVVXVtWdwInA/uPW2R84vhpnAxsl2azDmiRJklYKXYawLYBrBh4vbtse6DqSJEkjZ60Ot50J2mo51iHJwTSHKwFuTXLZDGvT6NsEuL7vIrT88v6+K5Cm5e+ZVdws/Z7ZZrIFXYawxcBWA4+3BK5djnWoqmOAY1Z0gRpdSRZU1fy+65A0uvw9o5nq8nDkecC2SR6RZG3gQODkceucDLykPUvyycDNVXVdhzVJkiStFDrrCauqu5O8FvgmsCZwXFVdnOTV7fKjgVOBfYArgNuAl3dVjyRJ0sokVfcbgiWt8pIc3B7GlqRO+HtGM2UIkyRJ6oGXLZIkSeqBIUyrjCTLkpyf5IIki5Ls3ndNklZeSbZM8rUklye5MslRSR60gvex5+DvoiSvTvKSFbDdq5JsMtPtaOVmCNOq5PaqekJV7QS8FXhf3wVJWjklCfBV4KSqGrs83rrAB1bwrvYE/hDCquroqjp+Be9DI6rLecKkLm0A3AiQZD3ga8BDgDnA26vqa0n+CPgizfxzawL/UlVfSLILcDiwHs1Eiy9zahRp5Pw58Puq+iRAVS1L8kbg6iSXA4+rqtcCJPlv4INV9YMkHwOeRBPYvlxVh7XrXAV8GngWze+ZvwJ+D7waWJbkRcDrgL2AW4HP08wAMObxwCNpZgI4Gti6bT+0qn6YZGPgBGAucC4TT2auEWMI06pk3STnA+sAm9H8koXmF+Fzqmpp231/dpKTgWcC11bVvgBJNkwyBzgS2L+qliR5AfAe4BWz/FokdWt7YOFgQ/s74iqm/r/vn6rqt0nWBL6bZMequrBddn1V7Zzk/wJvrqpXJjkauLWqPgiQZK92X9cCT2jbDgH+tKquTvJ54ENVdWaSrWmmcfpj4DDgzKp6V5J9ufcqMRphhjCtSm6vqicAJNkNOD7JDjR/Mb43ydOAe2iuP/pw4CLgg0neD/x3VZ3Rrr8D8O3maAVrAvaCSaMnTHAZPKbvYXp+e6m8tWj+2NsOGAthX21/LgSeO1QRyR7AK4Gntk1PB7Zrf/8AbJBkfeBpY9usqlOS3DjM9rVqM4RplVRVZ7W9XnNpJvydC+xSVXe1f+muU1U/aw897gO8L8m3gP8CLq6q3fqqXdKsuBh43mBDkg1o/kC7AXjMwKJ12uWPAN4MPKmqbkzyqbFlrTvan8sY4v/PJJsBxwLPrqpb2+Y1gN2q6vZx68LEoVEjzIH5WiUleRxNL9YNwIbAb9oA9me0F0tNsjlwW1V9FvggsDNwGTC37UkjyZwk2/fxGiR16rvAg8fOVGwPL/47cBTwC+AJSdZIshWwa/ucDYDfATcneTiw9xD7uQVYf3xjO/Thi8A/VtXPBhZ9C3jtwHpPaO+eDrywbdubZoyrRpwhTKuSddspKs4HvgC8tKqWAZ8D5idZQPNL7Kft+o8Hzm3X/yfg3VV1J3AA8P4kFwDnM3Bmk6TRUM1M5M8BDmgH4t8A3FNV7wF+SBPELqL5A21R+5wLgB/T9KId1643na8Dz2l/Nz11oH13mgH+7xz7vdX+Yfh6mt9XFya5hGZgP8A7gaclWQT8BfC/M3j5WkU4Y74kaeS1c3mdADy3qhZOt740GwxhkiRJPfBwpCRJUg8MYZIkST0whEmSJPXAECZJktQDQ5ikkZKkknxm4PFaSZa01wd8INu5qp0QeEbrSNJkDGGSRs3vgB2SrNs+fgbwyx7rkaQJGcIkjaJvAPu29w+imR8KgCQPTXJSO1nm2Ul2bNs3TvKtJD9O8nEGrjGY5EVJzm0n3Px4O/u6JM2IIUzSKDoRODDJOsCOwDkDy94J/LiqdgTeBhzfth8GnFlVTwROBrYGSPLHwAuAPdoLyC+jvbyMJM2EF/CWNHKq6sIk82h6wU4dt/gptBd2rqrvtT1gGwJPA57btp+S5MZ2/b2AXYDz2ossrwv8pvMXIWnkGcIkjaqTaa4LuCew8UB7Jli3xv0cFODTVfXWFVqdpNWehyMljarjgHdV1UXj2k+nPZyYZE/g+qpaOq59b+Ah7frfpbkI9MPaZQ9Nsk3n1UsaefaESRpJVbUYOGKCRe8APpnkQuA24KVt+zuBE5IsAk4D/rfdziVJ3g58K8kawF3AIcDV3b4CSaPOC3hLkiT1wMORkiRJPTCESZIk9cAQJkmS1ANDmCRJUg8MYZIkST0whEmSJPXAECZJktQDQ5gkSVIP/j+gVafqeH100QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = ['Base', 'Quantized']\n",
    "inference_times = [2.37, 2.44]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(models, inference_times)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Word error rate(WER)')\n",
    "plt.title('WER Comparison: Base and Quantized')\n",
    "plt.ylim(0, max(inference_times) * 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55952338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER difference between the quantized model and the base model is 2.87%\n"
     ]
    }
   ],
   "source": [
    "percentage_diff = ((2.44 - 2.37) / 2.44) * 100\n",
    "print(f'WER difference between the quantized model and the base model is {round(percentage_diff, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d8e306",
   "metadata": {},
   "source": [
    "## Base vs Quantized Model Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b5f233d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPU'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "ort.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e91dc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_sess_base = ort.InferenceSession('asr-wav2vec2-librispeech-finetuned-base.onnx')\n",
    "ort_sess_quantized = ort.InferenceSession('asr-wav2vec2-librispeech-qat.onnx', providers=['TensorrtExecutionProvider'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56045837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy\n",
    "\n",
    "def compute_inference_time(ort_sess, num_samples):\n",
    "    model_time = 0\n",
    "    with torch.no_grad():\n",
    "        for single_batch in data[\"test\"].select(range(num_samples)):\n",
    "            input_values = torch.tensor(single_batch[\"input_values\"], device=\"cpu\").unsqueeze(0)\n",
    "            input_values = input_values.clone().detach().numpy().astype(numpy.float32)\n",
    "            starttime = time.time()\n",
    "            ort_sess.run(['modelOutput'], {'modelInput': input_values})\n",
    "            model_time += time.time() - starttime\n",
    "    return round(model_time/num_samples, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3ec0d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 50\n",
    "\n",
    "base_model_time = compute_inference_time(ort_sess_base, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6493183a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Fine-Tuned Model inference time 0.45936726 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f'Base Fine-Tuned Model inference time {base_model_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a68e55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_time = compute_inference_time(ort_sess_quantized, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "069db4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization Aware Trained Model inference time 0.02008545 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f'Quantization Aware Trained Model inference time {quantized_model_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f143877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = ['Base', 'Quantized']\n",
    "inference_times = [base_model_time, quantized_model_time]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(models, inference_times)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Inference Time (seconds)')\n",
    "plt.title('Inference Time Comparison: Base Fine-Tuned and Quantization Aware Trained')\n",
    "plt.ylim(0, max(inference_times) * 1.1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
