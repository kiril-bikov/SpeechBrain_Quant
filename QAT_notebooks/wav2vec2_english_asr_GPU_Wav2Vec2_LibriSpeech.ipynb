{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "590528f4",
   "metadata": {},
   "source": [
    "# Training-Aware Quantization of SpeechBrain Wav2Vec2 on GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91676ae2",
   "metadata": {},
   "source": [
    "This notebook provides code for training-aware quantization of the speechbrain/asr-wav2vec2-librispeech model. We compare fine-tuned base model and quantization-aware trained model on the librispeech dataset. Training is performed for two epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ba4a75",
   "metadata": {},
   "source": [
    "First, we define the ASR class and write helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59b081d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import logging\n",
    "import speechbrain as sb\n",
    "from speechbrain.utils.distributed import run_on_main, if_main_process\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "from pathlib import Path\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ASR(sb.Brain):\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"\"\"Forward computations from the waveform batches to the output probabilities.\"\"\"\n",
    "        batch = batch.to(self.device)\n",
    "        wavs, wav_lens = batch.sig\n",
    "        wavs, wav_lens = wavs.to(self.device), wav_lens.to(self.device)\n",
    "        if hasattr(self.modules, \"downsampler\"):\n",
    "            wavs = self.modules.downsampler(wavs)\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            if hasattr(self.modules, \"env_corrupt\"):\n",
    "                wavs_noise = self.modules.env_corrupt(wavs, wav_lens)\n",
    "                wavs = torch.cat([wavs, wavs_noise], dim=0)\n",
    "                wav_lens = torch.cat([wav_lens, wav_lens])\n",
    "\n",
    "            if hasattr(self.hparams, \"augmentation\"):\n",
    "                wavs = self.hparams.augmentation(wavs, wav_lens)\n",
    "\n",
    "        if hasattr(self.modules, \"extractor\"):\n",
    "            latents = self.modules.extractor(wavs)\n",
    "            feats = self.modules.encoder_wrapper(latents, wav_lens=wav_lens)[\n",
    "                \"embeddings\"\n",
    "            ]\n",
    "        else:\n",
    "            feats = self.modules.wav2vec2(wavs, wav_lens)\n",
    "\n",
    "        x = self.modules.enc(feats)\n",
    "\n",
    "        p_tokens = None\n",
    "        logits = self.modules.ctc_lin(x)\n",
    "\n",
    "        if hasattr(self.hparams, \"upsampling\") and self.hparams.upsampling:\n",
    "            logits = logits.view(\n",
    "                logits.shape[0], -1, self.hparams.output_neurons\n",
    "            )\n",
    "\n",
    "        p_ctc = self.hparams.log_softmax(logits)\n",
    "        if stage == sb.Stage.VALID or (\n",
    "            stage == sb.Stage.TEST and not self.hparams.use_language_modelling\n",
    "        ):\n",
    "\n",
    "            p_tokens = sb.decoders.ctc_greedy_decode(\n",
    "                p_ctc, wav_lens, blank_id=self.hparams.blank_index\n",
    "            )\n",
    "        return p_ctc, wav_lens, p_tokens\n",
    "\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "        \"\"\"Computes the loss (CTC+NLL) given predictions and targets.\"\"\"\n",
    "\n",
    "        p_ctc, wav_lens, predicted_tokens = predictions\n",
    "\n",
    "        ids = batch.id\n",
    "        tokens, tokens_lens = batch.tokens\n",
    "\n",
    "        if hasattr(self.modules, \"env_corrupt\") and stage == sb.Stage.TRAIN:\n",
    "            tokens = torch.cat([tokens, tokens], dim=0)\n",
    "            tokens_lens = torch.cat([tokens_lens, tokens_lens], dim=0)\n",
    "\n",
    "        loss_ctc = self.hparams.ctc_cost(p_ctc, tokens, wav_lens, tokens_lens)\n",
    "        loss = loss_ctc\n",
    "\n",
    "        if stage == sb.Stage.VALID:\n",
    "            predicted_words = [\n",
    "                \"\".join(self.tokenizer.decode_ndim(utt_seq)).split(\" \")\n",
    "                for utt_seq in predicted_tokens\n",
    "            ]\n",
    "            target_words = [wrd.split(\" \") for wrd in batch.wrd]\n",
    "            self.wer_metric.append(ids, predicted_words, target_words)\n",
    "            self.cer_metric.append(ids, predicted_words, target_words)\n",
    "        if stage == sb.Stage.TEST:\n",
    "            if self.hparams.use_language_modelling:\n",
    "                predicted_words = []\n",
    "                for logs in p_ctc:\n",
    "                    text = decoder.decode(logs.detach().cpu().numpy())\n",
    "                    predicted_words.append(text.split(\" \"))\n",
    "            else:\n",
    "                predicted_words = [\n",
    "                    \"\".join(self.tokenizer.decode_ndim(utt_seq)).split(\" \")\n",
    "                    for utt_seq in predicted_tokens\n",
    "                ]\n",
    "            target_words = [wrd.split(\" \") for wrd in batch.wrd]\n",
    "            self.wer_metric.append(ids, predicted_words, target_words)\n",
    "            self.cer_metric.append(ids, predicted_words, target_words)\n",
    "        return loss\n",
    "\n",
    "    def fit_batch(self, batch):\n",
    "        should_step = self.step % self.grad_accumulation_factor == 0\n",
    "\n",
    "        if self.auto_mix_prec:\n",
    "            self.wav2vec_optimizer.zero_grad()\n",
    "            self.model_optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                with self.no_sync():\n",
    "                    outputs = self.compute_forward(batch, sb.Stage.TRAIN)\n",
    "                loss = self.compute_objectives(outputs, batch, sb.Stage.TRAIN)\n",
    "            with self.no_sync(not should_step):\n",
    "                self.scaler.scale(\n",
    "                    loss / self.grad_accumulation_factor\n",
    "                ).backward()\n",
    "            if should_step:\n",
    "                if not self.hparams.freeze_wav2vec:\n",
    "                    self.scaler.unscale_(self.wav2vec_optimizer)\n",
    "                self.scaler.unscale_(self.model_optimizer)\n",
    "                if self.check_gradients(loss):\n",
    "                    self.scaler.step(self.wav2vec_optimizer)\n",
    "                    self.scaler.step(self.model_optimizer)\n",
    "                self.scaler.update()\n",
    "                self.optimizer_step += 1\n",
    "        else:\n",
    "            with self.no_sync():\n",
    "                outputs = self.compute_forward(batch, sb.Stage.TRAIN)\n",
    "            loss = self.compute_objectives(outputs, batch, sb.Stage.TRAIN)\n",
    "            (loss / self.grad_accumulation_factor).backward()\n",
    "            if should_step:\n",
    "                if self.check_gradients(loss):\n",
    "                    self.wav2vec_optimizer.step()\n",
    "                    self.model_optimizer.step()\n",
    "                self.wav2vec_optimizer.zero_grad()\n",
    "                self.model_optimizer.zero_grad()\n",
    "                self.optimizer_step += 1\n",
    "\n",
    "        return loss.detach().cpu()\n",
    "\n",
    "    def on_stage_start(self, stage, epoch):\n",
    "        \"\"\"Gets called at the beginning of each epoch\"\"\"\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.cer_metric = self.hparams.cer_computer()\n",
    "            self.wer_metric = self.hparams.error_rate_computer()\n",
    "\n",
    "    def on_stage_end(self, stage, stage_loss, epoch):\n",
    "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
    "        stage_stats = {\"loss\": stage_loss}\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            self.train_stats = stage_stats\n",
    "        else:\n",
    "            stage_stats[\"CER\"] = self.cer_metric.summarize(\"error_rate\")\n",
    "            stage_stats[\"WER\"] = self.wer_metric.summarize(\"error_rate\")\n",
    "\n",
    "        if stage == sb.Stage.VALID:\n",
    "            old_lr_model, new_lr_model = self.hparams.lr_annealing_model(\n",
    "                stage_stats[\"loss\"]\n",
    "            )\n",
    "            old_lr_wav2vec, new_lr_wav2vec = self.hparams.lr_annealing_wav2vec(\n",
    "                stage_stats[\"loss\"]\n",
    "            )\n",
    "            sb.nnet.schedulers.update_learning_rate(\n",
    "                self.model_optimizer, new_lr_model\n",
    "            )\n",
    "            sb.nnet.schedulers.update_learning_rate(\n",
    "                self.wav2vec_optimizer, new_lr_wav2vec\n",
    "            )\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                stats_meta={\n",
    "                    \"epoch\": epoch,\n",
    "                    \"lr_model\": old_lr_model,\n",
    "                    \"lr_wav2vec\": old_lr_wav2vec,\n",
    "                },\n",
    "                train_stats=self.train_stats,\n",
    "                valid_stats=stage_stats,\n",
    "            )\n",
    "            self.checkpointer.save_and_keep_only(\n",
    "                meta={\"WER\": stage_stats[\"WER\"]}, min_keys=[\"WER\"],\n",
    "            )\n",
    "        elif stage == sb.Stage.TEST:\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
    "                test_stats=stage_stats,\n",
    "            )\n",
    "            if if_main_process():\n",
    "                with open(self.hparams.test_wer_file, \"w\") as w:\n",
    "                    self.wer_metric.write_stats(w)\n",
    "\n",
    "    def init_optimizers(self):\n",
    "        \"Initializes the wav2vec2 optimizer and model optimizer\"\n",
    "        if hasattr(self.modules, \"extractor\"):\n",
    "            self.wav2vec_optimizer = self.hparams.wav2vec_opt_class(\n",
    "                self.modules.encoder_wrapper.parameters()\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.wav2vec_optimizer = self.hparams.wav2vec_opt_class(\n",
    "                self.modules.wav2vec2.parameters()\n",
    "            )\n",
    "\n",
    "        self.model_optimizer = self.hparams.model_opt_class(\n",
    "            self.hparams.model.parameters()\n",
    "        )\n",
    "\n",
    "        if self.checkpointer is not None:\n",
    "            self.checkpointer.add_recoverable(\n",
    "                \"wav2vec_opt\", self.wav2vec_optimizer\n",
    "            )\n",
    "            self.checkpointer.add_recoverable(\"modelopt\", self.model_optimizer)\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        self.wav2vec_optimizer.zero_grad(set_to_none)\n",
    "        self.model_optimizer.zero_grad(set_to_none)\n",
    "\n",
    "\n",
    "def dataio_prepare(hparams):\n",
    "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
    "    It also defines the data processing pipeline through user-defined functions.\"\"\"\n",
    "    data_folder = hparams[\"data_folder\"]\n",
    "\n",
    "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
    "        csv_path=hparams[\"train_csv\"], replacements={\"data_root\": data_folder},\n",
    "    )\n",
    "\n",
    "    if hparams[\"sorting\"] == \"ascending\":\n",
    "        train_data = train_data.filtered_sorted(sort_key=\"duration\")\n",
    "        hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "    elif hparams[\"sorting\"] == \"descending\":\n",
    "        train_data = train_data.filtered_sorted(\n",
    "            sort_key=\"duration\", reverse=True\n",
    "        )\n",
    "\n",
    "        hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "    elif hparams[\"sorting\"] == \"random\":\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            \"sorting must be random, ascending or descending\"\n",
    "        )\n",
    "\n",
    "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
    "        csv_path=hparams[\"valid_csv\"], replacements={\"data_root\": data_folder},\n",
    "    )\n",
    "    valid_data = valid_data.filtered_sorted(sort_key=\"duration\")\n",
    "\n",
    "    test_datasets = {}\n",
    "    for csv_file in hparams[\"test_csv\"]:\n",
    "        name = Path(csv_file).stem\n",
    "        test_datasets[name] = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
    "            csv_path=csv_file, replacements={\"data_root\": data_folder}\n",
    "        )\n",
    "        test_datasets[name] = test_datasets[name].filtered_sorted(\n",
    "            sort_key=\"duration\"\n",
    "        )\n",
    "\n",
    "    datasets = [train_data, valid_data] + [i for k, i in test_datasets.items()]\n",
    "\n",
    "    @sb.utils.data_pipeline.takes(\"wav\")\n",
    "    @sb.utils.data_pipeline.provides(\"sig\")\n",
    "    def audio_pipeline(wav):\n",
    "        sig = sb.dataio.dataio.read_audio(wav)\n",
    "        return sig\n",
    "\n",
    "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
    "    label_encoder = sb.dataio.encoder.CTCTextEncoder()\n",
    "\n",
    "    @sb.utils.data_pipeline.takes(\"wrd\")\n",
    "    @sb.utils.data_pipeline.provides(\n",
    "        \"wrd\", \"char_list\", \"tokens_list\", \"tokens\"\n",
    "    )\n",
    "    def text_pipeline(wrd):\n",
    "        yield wrd\n",
    "        char_list = list(wrd)\n",
    "        yield char_list\n",
    "        tokens_list = label_encoder.encode_sequence(char_list)\n",
    "        yield tokens_list\n",
    "        tokens = torch.LongTensor(tokens_list)\n",
    "        yield tokens\n",
    "\n",
    "    sb.dataio.dataset.add_dynamic_item(datasets, text_pipeline)\n",
    "\n",
    "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
    "    special_labels = {\n",
    "        \"blank_label\": hparams[\"blank_index\"],\n",
    "    }\n",
    "    label_encoder.load_or_create(\n",
    "        path=lab_enc_file,\n",
    "        from_didatasets=[train_data],\n",
    "        output_key=\"char_list\",\n",
    "        special_labels=special_labels,\n",
    "        sequence_input=True,\n",
    "    )\n",
    "\n",
    "    sb.dataio.dataset.set_output_keys(\n",
    "        datasets, [\"id\", \"sig\", \"wrd\", \"char_list\", \"tokens\"],\n",
    "    )\n",
    "\n",
    "    return train_data, valid_data, test_datasets, label_encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44db9015",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed7b444b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad74a8ec35642c5b5609359cc7ee2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab491874fd98496482beb22fd890c576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea1ff5a52ac42669d7a511a31d9e51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration LibriSpeech-49d9aec4ade7f44e\n",
      "Reusing dataset text (/home/kmb85/.cache/huggingface/datasets/text/LibriSpeech-49d9aec4ade7f44e/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1871759fde4b6bbd83010dc29101d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "data = load_dataset(\"/home/kmb85/SpeechBrainQuant/LibriSpeech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72159ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_data import parse_to_json\n",
    "import json\n",
    "import os\n",
    "\n",
    "parse_to_json(\"./LibriSpeech/dev-clean\")\n",
    "os.rename('data.json', 'dev-clean.json')\n",
    "with open('/home/kmb85/SpeechBrainQuant/dev-clean.json', 'r') as file:\n",
    "    dev_clean_data = json.load(file)\n",
    "\n",
    "parse_to_json(\"./LibriSpeech/test-clean\")\n",
    "os.rename('data.json', 'test-clean.json')\n",
    "with open('/home/kmb85/SpeechBrainQuant/test-clean.json', 'r') as file:\n",
    "    test_clean_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f474fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_clean_dict = {}\n",
    "for key, value in dev_clean_data.items():\n",
    "    new_key = key.replace('-', '')\n",
    "    dev_clean_dict[new_key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1dd7a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean_dict = {}\n",
    "for key, value in test_clean_data.items():\n",
    "    new_key = key.replace('-', '')\n",
    "    test_clean_dict[new_key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b83c09d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import resampy\n",
    "\n",
    "def flac_to_array_with_sampling_rate(file_path, target_sr=16000):\n",
    "    data, sr = sf.read(file_path, dtype='float32')\n",
    "\n",
    "    if sr != target_sr:\n",
    "        data = resampy.resample(data, sr, target_sr)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "947b0ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"]'\n",
    "\n",
    "def remove_special_characters_dev(batch):\n",
    "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower() + \" \"\n",
    "    trimmed_string = batch[\"text\"].strip()\n",
    "    words = trimmed_string.split()\n",
    "    new_text = ' '.join(words[1:])\n",
    "    batch[\"text\"] = new_text\n",
    "    batch[\"input_values\"] = flac_to_array_with_sampling_rate(dev_clean_dict[words[0]]['file_path'])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1a8c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['validation'] = data['validation'].map(remove_special_characters_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da82949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"]'\n",
    "\n",
    "def remove_special_characters_test(batch):\n",
    "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower() + \" \"\n",
    "    trimmed_string = batch[\"text\"].strip()\n",
    "    words = trimmed_string.split()\n",
    "    new_text = ' '.join(words[1:])\n",
    "    batch[\"text\"] = new_text\n",
    "    batch[\"input_values\"] = flac_to_array_with_sampling_rate(test_clean_dict[words[0]]['file_path'])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d7b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['test'] = data['test'].map(remove_special_characters_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c187860",
   "metadata": {},
   "source": [
    "## Base Model Traing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10f3ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_file = 'hparams/train_hf_wav2vec_base.yaml'\n",
    "run_opts = {\"device\":\"cuda\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee711739",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.utils.distributed.ddp_init_group(run_opts)\n",
    "\n",
    "with open(hparams_file) as fin:\n",
    "    hparams = load_hyperpyyaml(fin, '')\n",
    "\n",
    "sb.create_experiment_directory(\n",
    "    experiment_directory=hparams[\"output_folder\"],\n",
    "    hyperparams_to_save=hparams_file,\n",
    "    overrides='',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad5954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from librispeech_prepare import prepare_librispeech\n",
    "\n",
    "run_on_main(\n",
    "    prepare_librispeech,\n",
    "    kwargs={\n",
    "        \"data_folder\": hparams[\"data_folder\"],\n",
    "        \"tr_splits\": hparams[\"train_splits\"],\n",
    "        \"dev_splits\": hparams[\"dev_splits\"],\n",
    "        \"te_splits\": hparams[\"test_splits\"],\n",
    "        \"save_folder\": hparams[\"output_folder\"],\n",
    "        \"merge_lst\": hparams[\"train_splits\"],\n",
    "        \"merge_name\": \"train.csv\",\n",
    "        \"skip_prep\": hparams[\"skip_prep\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8cf4a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.dataio.encoder - Moving label 'T' from index 0, because '<blank>' was put at its place.\n",
      "speechbrain.dataio.encoder - Load called, but CTCTextEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_datasets, label_encoder = dataio_prepare(\n",
    "    hparams\n",
    ")\n",
    "\n",
    "if hasattr(hparams, \"use_language_modelling\"):\n",
    "    if hparams[\"use_language_modelling\"]:\n",
    "        try:\n",
    "            from pyctcdecode import build_ctcdecoder\n",
    "        except ImportError:\n",
    "            err_msg = \"Optional dependencies must be installed to use pyctcdecode.\\n\"\n",
    "            err_msg += \"Install using `pip install kenlm pyctcdecode`.\\n\"\n",
    "            raise ImportError(err_msg)\n",
    "\n",
    "        ind2lab = label_encoder.ind2lab\n",
    "        labels = [ind2lab[x] for x in range(len(ind2lab))]\n",
    "        labels = [\"\"] + labels[\n",
    "            1:\n",
    "        ]\n",
    "        decoder = build_ctcdecoder(\n",
    "            labels,\n",
    "            kenlm_model_path=hparams[\"ngram_lm_path\"],  # .arpa or .bin\n",
    "            alpha=0.5,  # Default by KenLM\n",
    "            beta=1.0,  # Default by KenLM\n",
    "        )\n",
    "else:\n",
    "    hparams[\"use_language_modelling\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a686a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.core - Info: auto_mix_prec arg from hparam file is used\n",
      "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
      "speechbrain.core - 2.1M trainable parameters in ASR\n"
     ]
    }
   ],
   "source": [
    "asr_brain_base = ASR(\n",
    "    modules=hparams[\"modules\"],\n",
    "    hparams=hparams,\n",
    "    run_opts=run_opts,\n",
    "    checkpointer=hparams[\"checkpointer\"],\n",
    ")\n",
    "\n",
    "if \"pretrainer\" in hparams.keys():\n",
    "    run_on_main(hparams[\"pretrainer\"].collect_files)\n",
    "    hparams[\"pretrainer\"].load_collected('cuda')\n",
    "\n",
    "asr_brain_base.tokenizer = label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efb4534",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "asr_brain_base.fit(\n",
    "    asr_brain_base.hparams.epoch_counter,\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_loader_kwargs=hparams[\"train_dataloader_opts\"],\n",
    "    valid_loader_kwargs=hparams[\"valid_dataloader_opts\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f356297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "if not os.path.exists(hparams[\"output_wer_folder\"]):\n",
    "    os.makedirs(hparams[\"output_wer_folder\"])\n",
    "\n",
    "for k in test_datasets.keys():\n",
    "    asr_brain_base.hparams.test_wer_file = os.path.join(\n",
    "        hparams[\"output_wer_folder\"], f\"wer_{k}.txt\"\n",
    "    )\n",
    "    asr_brain_base.evaluate(\n",
    "        test_datasets[k],\n",
    "        test_loader_kwargs=hparams[\"test_dataloader_opts\"],\n",
    "        min_key=\"WER\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8456bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(asr_brain_base.modules.wav2vec2.model,\n",
    "     torch.tensor(data['validation'][:1]['input_values']).cuda(),\n",
    "     f=\"asr-wav2vec2-librispeech-finetuned-base.onnx\",\n",
    "     export_params=True,\n",
    "     opset_version=17,\n",
    "     do_constant_folding=True,\n",
    "     input_names = ['modelInput'],\n",
    "     output_names = ['modelOutput'],\n",
    "     dynamic_axes={'modelInput' : {0 : 'batch_size', 1: 'batch_size'},\n",
    "     'modelOutput' : {0 : 'batch_size'}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38745cd0",
   "metadata": {},
   "source": [
    "## Quantization-Aware Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8f0d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_file = 'hparams/train_hf_wav2vec_quantized.yaml'\n",
    "run_opts = {\"device\":\"cuda\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2093688e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3309a1050f7243af81226aa848c3c746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/158 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758bdb6f1925409bb44d4758580e36ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c16158457949fcaeee349e0e79b71a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kmb85/.local/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-large-960h-lv60-self were not used when initializing Wav2Vec2Model: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 is frozen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: results/train_wav2vec2_char/1987\n"
     ]
    }
   ],
   "source": [
    "sb.utils.distributed.ddp_init_group(run_opts)\n",
    "\n",
    "with open(hparams_file) as fin:\n",
    "    hparams = load_hyperpyyaml(fin, '')\n",
    "\n",
    "sb.create_experiment_directory(\n",
    "    experiment_directory=hparams[\"output_folder\"],\n",
    "    hyperparams_to_save=hparams_file,\n",
    "    overrides='',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad812fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from librispeech_prepare import prepare_librispeech\n",
    "\n",
    "run_on_main(\n",
    "    prepare_librispeech,\n",
    "    kwargs={\n",
    "        \"data_folder\": hparams[\"data_folder\"],\n",
    "        \"tr_splits\": hparams[\"train_splits\"],\n",
    "        \"dev_splits\": hparams[\"dev_splits\"],\n",
    "        \"te_splits\": hparams[\"test_splits\"],\n",
    "        \"save_folder\": hparams[\"output_folder\"],\n",
    "        \"merge_lst\": hparams[\"train_splits\"],\n",
    "        \"merge_name\": \"train.csv\",\n",
    "        \"skip_prep\": hparams[\"skip_prep\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6955b034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.dataio.encoder - Load called, but CTCTextEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_datasets, label_encoder = dataio_prepare(\n",
    "    hparams\n",
    ")\n",
    "\n",
    "if hasattr(hparams, \"use_language_modelling\"):\n",
    "    if hparams[\"use_language_modelling\"]:\n",
    "        try:\n",
    "            from pyctcdecode import build_ctcdecoder\n",
    "        except ImportError:\n",
    "            err_msg = \"Optional dependencies must be installed to use pyctcdecode.\\n\"\n",
    "            err_msg += \"Install using `pip install kenlm pyctcdecode`.\\n\"\n",
    "            raise ImportError(err_msg)\n",
    "\n",
    "        ind2lab = label_encoder.ind2lab\n",
    "        labels = [ind2lab[x] for x in range(len(ind2lab))]\n",
    "        labels = [\"\"] + labels[\n",
    "            1:\n",
    "        ]\n",
    "        decoder = build_ctcdecoder(\n",
    "            labels,\n",
    "            kenlm_model_path=hparams[\"ngram_lm_path\"],  # .arpa or .bin\n",
    "            alpha=0.5,  # Default by KenLM\n",
    "            beta=1.0,  # Default by KenLM\n",
    "        )\n",
    "else:\n",
    "    hparams[\"use_language_modelling\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "419fcfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.core - Info: auto_mix_prec arg from hparam file is used\n",
      "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
      "speechbrain.core - 2.1M trainable parameters in ASR\n"
     ]
    }
   ],
   "source": [
    "asr_brain_quantized = ASR(\n",
    "    modules=hparams[\"modules\"],\n",
    "    hparams=hparams,\n",
    "    run_opts=run_opts,\n",
    "    checkpointer=hparams[\"checkpointer\"],\n",
    ")\n",
    "\n",
    "if \"pretrainer\" in hparams.keys():\n",
    "    run_on_main(hparams[\"pretrainer\"].collect_files)\n",
    "    hparams[\"pretrainer\"].load_collected('cuda')\n",
    "\n",
    "asr_brain_quantized.tokenizer = label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a719757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0eff129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/kmb85/rds/hpc-work/huggingface'\n",
    "os.environ['HF_HOME']='/home/kmb85/rds/hpc-work/huggingface'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a83387f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import quant_modules\n",
    "from pytorch_quantization.tensor_quant import QuantDescriptor\n",
    "from pytorch_quantization import calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a6acf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_desc_input = QuantDescriptor(calib_method='max', num_bits=8, fake_quant=True)\n",
    "quant_nn.QuantConv1d.set_default_quant_desc_input(quant_desc_input)\n",
    "quant_nn.QuantLinear.set_default_quant_desc_input(quant_desc_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee72273f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a50d56ff0d40ee887e887e1ba3e14d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hyperparams.yaml:   0%|          | 0.00/1.78k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f333b8eeaa7f4e36a51760d34d44fcfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-large-960h-lv60-self were not used when initializing Wav2Vec2Model: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 is frozen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0108 14:06:04.263104 140341998555136 huggingface_wav2vec.py:146] speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 is frozen.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f1ba8193a746f988646cf91969635f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wav2vec2.ckpt:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d8f27ff97f4ef39aa3a8299744f795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "asr.ckpt:   0%|          | 0.00/8.53M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3e5efed8014a1a9f18d3583b4da7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.ckpt:   0%|          | 0.00/426 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from speechbrain.pretrained import EncoderASR\n",
    "\n",
    "quant_modules.initialize()\n",
    "model_full_quantized = EncoderASR.from_hparams(source=\"speechbrain/asr-wav2vec2-librispeech\", run_opts={\"device\":\"cuda\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3bdb0e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full_quantized.mods.encoder.wav2vec2.model.feature_extractor.conv_layers[0].conv._weight_quantizer.enable_calib()\n",
    "model_full_quantized.mods.encoder.wav2vec2.model.feature_extractor.conv_layers[0].conv._weight_quantizer.disable_quant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8ec83e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full_quantized = model_full_quantized.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99acb40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    for i in range(2000):\n",
    "            input_value = torch.tensor(data[\"validation\"][i:i+1][\"input_values\"], device=\"cuda\")\n",
    "            _ = model_full_quantized.mods.encoder.wav2vec2.model(input_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bcc152f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl - Load calibrated amax, shape=torch.Size([512, 1, 1]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0108 14:13:53.678715 140341998555136 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([512, 1, 1]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl - Call .cuda() if running on GPU after loading calibrated amax.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0108 14:13:53.681514 140341998555136 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl - Disable MaxCalibrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0108 14:13:53.682647 140341998555136 tensor_quantizer.py:174] Disable MaxCalibrator\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncoderASR(\n",
       "  (mods): ModuleDict(\n",
       "    (encoder): LengthsCapableSequential(\n",
       "      (wav2vec2): HuggingFaceWav2Vec2(\n",
       "        (model): Wav2Vec2Model(\n",
       "          (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "            (conv_layers): ModuleList(\n",
       "              (0): Wav2Vec2LayerNormConvLayer(\n",
       "                (conv): QuantConv1d(\n",
       "                  1, 512, kernel_size=(10,), stride=(5,)\n",
       "                  (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                  (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0071, 0.6079](512) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                )\n",
       "                (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "                (conv): QuantConv1d(\n",
       "                  512, 512, kernel_size=(3,), stride=(2,)\n",
       "                  (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                  (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                )\n",
       "                (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "                (conv): QuantConv1d(\n",
       "                  512, 512, kernel_size=(2,), stride=(2,)\n",
       "                  (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                  (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                )\n",
       "                (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (feature_projection): Wav2Vec2FeatureProjection(\n",
       "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (projection): QuantLinear(\n",
       "              in_features=512, out_features=1024, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "            (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "              (conv): QuantConv1d(\n",
       "                1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "                (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "              )\n",
       "              (padding): Wav2Vec2SamePadLayer()\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layers): ModuleList(\n",
       "              (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "                (attention): Wav2Vec2Attention(\n",
       "                  (k_proj): QuantLinear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                    (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                  )\n",
       "                  (v_proj): QuantLinear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                    (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                  )\n",
       "                  (q_proj): QuantLinear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                    (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                  )\n",
       "                  (out_proj): QuantLinear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                    (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (feed_forward): Wav2Vec2FeedForward(\n",
       "                  (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (intermediate_dense): QuantLinear(\n",
       "                    in_features=1024, out_features=4096, bias=True\n",
       "                    (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                    (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (output_dense): QuantLinear(\n",
       "                    in_features=4096, out_features=1024, bias=True\n",
       "                    (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                    (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "                  )\n",
       "                  (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (enc): VanillaNN(\n",
       "        (linear): Linear(\n",
       "          (w): QuantLinear(\n",
       "            in_features=1024, out_features=1024, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "        )\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (linear_0): Linear(\n",
       "          (w): QuantLinear(\n",
       "            in_features=1024, out_features=1024, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "        )\n",
       "        (act_0): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (ctc_lin): Linear(\n",
       "        (w): QuantLinear(\n",
       "          in_features=1024, out_features=31, bias=True\n",
       "          (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finalize calibration\n",
    "\n",
    "model_full_quantized.mods.encoder.wav2vec2.model.feature_extractor.conv_layers[0].conv._weight_quantizer.load_calib_amax()\n",
    "model_full_quantized.mods.encoder.wav2vec2.model.feature_extractor.conv_layers[0].conv._weight_quantizer.disable_calib()\n",
    "model_full_quantized.mods.encoder.wav2vec2.model.feature_extractor.conv_layers[0].conv._weight_quantizer.enable_quant()\n",
    "\n",
    "model_full_quantized.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1b077bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_brain_quantized.modules.wav2vec2.model.feature_extractor.conv_layers[0].conv = model_full_quantized.mods.encoder.wav2vec2.model.feature_extractor.conv_layers[0].conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f3a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_brain_quantized.fit(\n",
    "    asr_brain_quantized.hparams.epoch_counter,\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_loader_kwargs=hparams[\"train_dataloader_opts\"],\n",
    "    valid_loader_kwargs=hparams[\"valid_dataloader_opts\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e26ae65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - Epoch loaded: 2 - test loss: 4.83e-02, test CER: 6.08e-01, test WER: 3.64\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "if not os.path.exists(hparams[\"output_wer_folder\"]):\n",
    "    os.makedirs(hparams[\"output_wer_folder\"])\n",
    "\n",
    "for k in test_datasets.keys():\n",
    "    asr_brain_quantized.hparams.test_wer_file = os.path.join(\n",
    "        hparams[\"output_wer_folder\"], f\"wer_{k}.txt\"\n",
    "    )\n",
    "    asr_brain_quantized.evaluate(\n",
    "        test_datasets[k],\n",
    "        test_loader_kwargs=hparams[\"test_dataloader_opts\"],\n",
    "        min_key=\"WER\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ba611da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kmb85/.local/lib/python3.10/site-packages/pytorch_quantization/tensor_quant.py:257: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if amax.numel() == 1:\n",
      "/home/kmb85/.local/lib/python3.10/site-packages/pytorch_quantization/tensor_quant.py:260: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  axis = amax.shape.index(amax.numel())\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(asr_brain_quantized.modules.wav2vec2.model,\n",
    "     torch.tensor(data['validation'][:1]['input_values']).cuda(),\n",
    "     f=\"asr-wav2vec2-librispeech-qat.onnx\",\n",
    "     export_params=True,\n",
    "     opset_version=17,\n",
    "     do_constant_folding=True,\n",
    "     input_names = ['modelInput'],\n",
    "     output_names = ['modelOutput'],\n",
    "     dynamic_axes={'modelInput' : {0 : 'batch_size', 1: 'batch_size'},\n",
    "     'modelOutput' : {0 : 'batch_size'}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fca267",
   "metadata": {},
   "source": [
    "## Base vs Quantized Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0227e556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = ['Base', 'Quantized']\n",
    "inference_times = [2.37, 3.64]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(models, inference_times)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Word error rate(WER)')\n",
    "plt.title('WER Comparison: Base and Quantized')\n",
    "plt.ylim(0, max(inference_times) * 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55952338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER difference between the quantized model and the base model is 34.89%\n"
     ]
    }
   ],
   "source": [
    "percentage_diff = ((3.64 - 2.37) / 3.64) * 100\n",
    "print(f'WER difference between the quantized model and the base model is {round(percentage_diff, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d8e306",
   "metadata": {},
   "source": [
    "## Base vs Quantized Model Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b5f233d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPU'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "ort.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e91dc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_sess_base = ort.InferenceSession('asr-wav2vec2-librispeech-finetuned-base.onnx')\n",
    "ort_sess_quantized = ort.InferenceSession('asr-wav2vec2-librispeech-qat.onnx', providers=['TensorrtExecutionProvider'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56045837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy\n",
    "\n",
    "def compute_inference_time(ort_sess, num_samples):\n",
    "    model_time = 0\n",
    "    with torch.no_grad():\n",
    "        for single_batch in data[\"test\"].select(range(num_samples)):\n",
    "            input_values = torch.tensor(single_batch[\"input_values\"], device=\"cpu\").unsqueeze(0)\n",
    "            input_values = input_values.clone().detach().numpy().astype(numpy.float32)\n",
    "            starttime = time.time()\n",
    "            ort_sess.run(['modelOutput'], {'modelInput': input_values})\n",
    "            model_time += time.time() - starttime\n",
    "    return round(model_time/num_samples, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3ec0d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 50\n",
    "\n",
    "base_model_time = compute_inference_time(ort_sess_base, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6493183a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Fine-Tuned Model inference time 0.45936726 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f'Base Fine-Tuned Model inference time {base_model_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a68e55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_time = compute_inference_time(ort_sess_quantized, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "069db4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization Aware Trained Model inference time 0.02008545 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f'Quantization Aware Trained Model inference time {quantized_model_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f143877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = ['Base', 'Quantized']\n",
    "inference_times = [base_model_time, quantized_model_time]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(models, inference_times)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Inference Time (seconds)')\n",
    "plt.title('Inference Time Comparison: Base Fine-Tuned and Quantization Aware Trained')\n",
    "plt.ylim(0, max(inference_times) * 1.1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
